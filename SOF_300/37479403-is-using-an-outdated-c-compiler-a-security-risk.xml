<?xml version="1.0" encoding="utf-8"?>
<post><title>Is using an outdated C compiler a security risk? - Stack Overflow</title><question><text><div class="post-text" itemprop="text">
<p>We have some build systems in production which no one cares about and these machines run ancient versions of GCC like GCC 3 or GCC 2.</p>
<p>And I can't persuade the management to upgrade it to a more recent: they say, "if ain't broke, don't fix it". </p>
<p>Since we maintain a very old code base (written in the 80s), this C89 code compiles just fine on these compilers. </p>
<p>But I'm not sure it is good idea to use these old stuff.</p>
<p>My question is:</p>
<p>Can using an old C compiler compromise the security of the compiled program?  </p>
<p>UPDATE:</p>
<p>The same code is built by Visual Studio 2008 for Windows targets, and MSVC doesn't support C99 or C11 yet (I don't know if newer MSVC does), and I can build it on my Linux box using the latest GCC. So if we would just drop in a newer GCC it would probably build just as fine as before.</p>
</div></text><author><a href="/users/58805/calmarius">Calmarius</a></author><comments><comment><text><span class="comment-copy">Interesting question - this might be worth a quick read as well - <a href="https://developers.slashdot.org/story/13/10/29/2150211/how-your-compiler-can-compromise-application-security" rel="nofollow noreferrer">developers.slashdot.org/story/13/10/29/2150211/…</a>  .. so newer compilers might also compromise security when optimizing.</span></text><author><a class="comment-user" href="/users/261981/neil" title="869 reputation">Neil</a></author></comment><comment><text><span class="comment-copy">It's worth noting that even old compiler versions may be supported by security backports. For gcc in partucular, <i>Note that starting with version 3.3.4, we provide bug releases for older release branches for those users who require a very high degree of stability.</i></span></text><author><a class="comment-user" href="/users/5725780/pipe" title="386 reputation">pipe</a></author></comment><comment><text><span class="comment-copy">I have no GCC knowledge, but does older compiler mean older C runtime, or the newest runtime is always used? If the old runtime is used, old vulnerabilities can be used as an argument to upgrade: <a href="https://www.cvedetails.com/vulnerability-list/vendor_id-72/product_id-767/GNU-Glibc.html" rel="nofollow noreferrer">cvedetails.com/vulnerability-list/vendor_id-72/product_id-76‌​7/…</a></span></text><author><a class="comment-user" href="/users/4304120/marian-spanik" title="667 reputation">Marian Spanik</a></author></comment><comment><text><span class="comment-copy">Do those old gcc versions support compiling to PIC/PIE for ASLR? Do they support stack canaries? W^X (NX)? If not, the lack of mitigations for vulnerabilities is a good reason to upgrade.</span></text><author><a class="comment-user" href="/users/3185968/eof" title="3,530 reputation">EOF</a></author></comment><comment><text><span class="comment-copy">IMO this is "too broad". Analyzing a specific piece of code with precise version of tools in toolchain could yield a precise answer.</span></text><author><a class="comment-user" href="/users/936986/oleg-v-volkov" title="14,560 reputation">Oleg V. Volkov</a></author></comment><comment><text><span class="comment-copy">Just looking at the warnings from gcc 4.x may immediately reveal a whole load of existing security holes you didn't know you had.</span></text><author><a class="comment-user" href="/users/476716/orangedog" title="14,990 reputation">OrangeDog</a></author></comment><comment><text><span class="comment-copy">To answer this question, one should really know the context. Where do those programs run, doing what, to whom/what, steered by whom/what, ...?</span></text><author><a class="comment-user" href="/users/694576/alk" title="48,453 reputation">alk</a></author></comment><comment><text><span class="comment-copy">Simple: read the known bugs of those old compilers and inject code that triggers them into the code base.</span></text><author><a class="comment-user" href="/users/3528438/user3528438" title="1,390 reputation">user3528438</a></author></comment><comment><text><span class="comment-copy">You're using GCC 3.x; I recently helped someone who was GCC 2.95.3 still.  Is your o/s as ancient as your compiler?  If so, there are risks associated with the antique o/s that are greater than the risks in the compilation.  I have considerable sympathy with what <a href="https://stackoverflow.com/users/5083516/plugwash">plugwash</a> <a href="http://stackoverflow.com/a/37484698/">says</a> w.r.t to modern compilers being almost unpleasantly aggressive in their optimization.</span></text><author><a class="comment-user" href="/users/15168/jonathan-leffler" title="472,035 reputation">Jonathan Leffler</a></author></comment><comment><text><span class="comment-copy">Using an outdated compiler will make you put more security risks in your program, because you don't have features like overflow-checked arithmetic.</span></text><author><a class="comment-user" href="/users/106104/immibis" title="29,452 reputation">immibis</a></author></comment><comment><text><span class="comment-copy">Is this system connected to any network at all?  Or is it competely stand alone ?</span></text><author><a class="comment-user" href="/users/3471739/criggie" title="134 reputation">Criggie</a></author></comment><comment><text><span class="comment-copy">@OrangeDog: Why gcc 4.x?  gcc6 is the current release series, and gcc 5 has been around for a while.  But yeah, fixing any problems identified by <code>-O3 -Wall -Wextra -fsanitize=undefined</code> with modern gcc and clang should help.</span></text><author><a class="comment-user" href="/users/224132/peter-cordes" title="54,745 reputation">Peter Cordes</a></author></comment><comment><text><span class="comment-copy">@PeterCordes seems you wait eight years for a major version update and three come along at once.</span></text><author><a class="comment-user" href="/users/476716/orangedog" title="14,990 reputation">OrangeDog</a></author></comment><comment><text><span class="comment-copy">@OrangeDog GCC has gone to marketing version numbers.  GCC 5 deserved a major version bump, because they changed the default C and C++ standards and the libstdc++ ABI.  GCC 6 should have been called 5.1.</span></text><author><a class="comment-user" href="/users/388520/zwol" title="75,820 reputation">zwol</a></author></comment><comment><text><span class="comment-copy">Related post at <a href="http://aviation.stackexchange.com/q/21602/3354">aviation stackexchange</a>. Unfortunately it was closed :( May be I made a mistake</span></text><author><a class="comment-user" href="/users/1870232/p0w" title="28,650 reputation">P0W</a></author></comment><comment><text><span class="comment-copy">Surely you can quickly check how much support for C99/C11 MSVS version have? But I can tell you that, whether or not it officially supports it, MSVS 2013 (+ probably 2012) has a lot of the useful stuff, like declarations after statements (so you need not declare before you are ready to initialise) and declaring variables in ``for`-loops. MSVS 2013 Code Analysis is also pretty useful (if you enable it!).</span></text><author><a class="comment-user" href="/users/4847772/pjtraill" title="900 reputation">PJTraill</a></author></comment></comments></question><answers><answer><text><div class="post-text" itemprop="text">
<p>Actually I would argue the opposite.</p>
<p>There are a number of cases where behaviour is undefined by the C standard but where it is obvious what would happen with a "dumb compiler" on a given platform. Cases like allowing a signed integer to overflow or accessing the same memory though variables of two different types.</p>
<p>Recent versions of gcc (and clang) have started treating these cases as optimisation opportunities not caring if they change how the binary behaves in the "undefined behaviour" condition.  This is very bad if your codebase was written by people who treated C like a "portable assembler". As time went on the optimisers have started looking at larger and larger chunks of code when doing these optimisations increasing the chance the binary will end up doing something other than "what a binary built by a dumb compiler" would do.</p>
<p>There are compiler switches to restore "traditional" behaviour (-fwrapv and -fno-strict-aliasing for the two I mentioned above) , but first you have to know about them.</p>
<p>While in principle a compiler bug could turn compliant code into a security hole I would consider the risk of this to be negligable in the grand scheme of things.</p>
</div></text><author><a href="/users/5083516/plugwash">plugwash</a></author><comments><comment><text><span class="comment-copy">But this argument works both ways. If a compiler has predictable  "undefined behaviour", then it can be arguably more easy to make malicious use of it...</span></text><author><a class="comment-user" href="/users/417197/andre" title="10,208 reputation">Andre</a></author></comment><comment><text><span class="comment-copy">@Andre The <i>compiled code</i> has predictable undefined behaviour anyway. That is, once the code has been compiled, any unpredictable behaviour is now predictable, in that particular compiled version.</span></text><author><a class="comment-user" href="/users/106104/immibis" title="29,452 reputation">immibis</a></author></comment><comment><text><span class="comment-copy"><code>people who treated C like a "portable assembler"</code> isn't it what C is though?</span></text><author><a class="comment-user" href="/users/1054649/max" title="1,650 reputation">Max</a></author></comment><comment><text><span class="comment-copy">@Max This answer warns precisely about the fact that the "portable assembler" notion is at least outdated in practice, due to modern optimizers. And I would argue that it never was conceptually correct to begin with.</span></text><author><a class="comment-user" href="/users/1892179/theodoros-chatzigiannakis" title="20,222 reputation">Theodoros Chatzigiannakis</a></author></comment><comment><text><span class="comment-copy">Older compilers are not necessarily "dumber" - an older GCC, I think it was 4, was extremely aggressive with strict aliasing rules and broke a lot of programs. It was toned done (or handling enhanced) in later versions.</span></text><author><span class="comment-user">user3995702</span></author></comment><comment><text><span class="comment-copy">No sympathy here for those who rely on undefined behaviour and later begin to reap what they sowed. This doesn't mean newer compilers are inherently less secure - it means the noncompliant code was a time bomb. Blame should be apportioned accordingly.</span></text><author><a class="comment-user" href="/users/2757035/underscore-d" title="2,022 reputation">underscore_d</a></author></comment><comment><text><span class="comment-copy">@underscore_d why would you blame code that predates the standard for not being compliant to the standard?</span></text><author><a class="comment-user" href="/users/3764814/lucas-trzesniewski" title="36,462 reputation">Lucas Trzesniewski</a></author></comment><comment><text><span class="comment-copy">@LucasTrzesniewski The 1st version of my comment had an exclusion for pre-standardisation code, but I forgot to put that back in after editing. Of course, one cannot blame such code for not adhering to a standard that didn't exist yet. I'm talking about post-Standard code that doesn't do the research and then tries to villify compiler writers for not handling UB the way some particular programmer likes - something that seems fairly common.</span></text><author><a class="comment-user" href="/users/2757035/underscore-d" title="2,022 reputation">underscore_d</a></author></comment><comment><text><span class="comment-copy">@underscore_d Blame is all fun and and games, but when someone has to put up dollars and man-hours to change things, the reality of having a body of code like that outweighs purity rather quickly.  The real question is not blame but the best path to move forward.</span></text><author><a class="comment-user" href="/users/2728148/cort-ammon" title="4,602 reputation">Cort Ammon</a></author></comment><comment><text><span class="comment-copy">@CortAmmon True, practical things are more important than ideology. If a middle ground can be found, or implementation-defined behaviour used, I can't complain. My point was reflexively blaming compilers for not defining UB hinders compromise by decreasing the s/n ratio of discussions... but now I wonder if my 1st comment might not be much better in that sense :)</span></text><author><a class="comment-user" href="/users/2757035/underscore-d" title="2,022 reputation">underscore_d</a></author></comment><comment><text><span class="comment-copy">@underscore_d: Undefined behaviour is most often just a plain bug (in your source code). But the compiler can translate it in different ways, and if it gets translated into something that is not a bug in the compiled code, then no amount of testing and customer exposure will find it.</span></text><author><a class="comment-user" href="/users/3255455/gnasher729" title="35,679 reputation">gnasher729</a></author></comment><comment><text><span class="comment-copy">These optimisations are by no means compiler bugs. They are certainly not compiler bugs <i>in principle</i> as you claim, even if you see them as being effectively bugs.</span></text><author><a class="comment-user" href="/users/575229/miles-rout" title="802 reputation">Miles Rout</a></author></comment><comment><text><span class="comment-copy">You can define a significant amount of undefined behaviour anyway - compiler flags can turn signed overflow into automatically wrapping or automatically trapping if you want to on all relevant compilers.</span></text><author><a class="comment-user" href="/users/575229/miles-rout" title="802 reputation">Miles Rout</a></author></comment><comment><text><span class="comment-copy">@gnasher729: A lot of behaviors that aren't defined by the Standard were defined by many C implementations prior to the Standard, and I've seen no evidence that the authors of the Standard didn't intend that behaviors that were reliably defined on quality pre-C89 implementations on a given platform should, whenever practical, be defined the same way on quality C89 implementations for the same platform.</span></text><author><a class="comment-user" href="/users/363751/supercat" title="45,690 reputation">supercat</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>There are risks in both courses of action.</p>
<hr/>
<p>Older compilers have the advantage of maturity, and whatever was broken in them has probably (but there's no guarantee) been worked around successfully.</p>
<p>In this case, a new compiler is a potential source of new bugs.</p>
<hr/>
<p>On the other hand, newer compilers come with <em>additional tooling</em>:</p>
<ul>
<li>GCC and Clang both now feature <em>sanitizers</em> which can instrument the runtime to detect undefined behaviors of various sorts (Chandler Carruth, of the Google Compiler team, claimed last year that he expects them to have reached full coverage)</li>
<li>Clang, at least, features <em>hardening</em>, for example <a href="http://clang.llvm.org/docs/ControlFlowIntegrity.html">Control Flow Integrity</a> is about detecting hi-jacks of control flow, there are also hardening implements to protect against stack smashing attacks (by separating the control-flow part of the stack from the data part); hardening features are generally low overhead (&lt; 1% CPU overhead)</li>
<li>Clang/LLVM is also working on <a href="http://llvm.org/docs/LibFuzzer.html">libFuzzer</a>, a tool to create instrumented fuzzing unit-tests that explore the input space of the function under test smartly (by tweaking the input to take not-as-yet explored execution paths)</li>
</ul>
<p>Instrumenting your binary with the sanitizers (Address Sanitizer, Memory Sanitizer or Undefined Behavior Sanitizer) and then fuzzing it (using <a href="https://en.wikipedia.org/wiki/American_fuzzy_lop_%28fuzzer%29">American Fuzzy Lop</a> for example) has uncovered vulnerabilities in a number of high-profile softwares, see for example this <a href="https://lwn.net/Articles/657959/">LWN.net article</a>.</p>
<p>Those new tools, and all future tools, are inaccessible to you unless you upgrade your compiler.</p>
<p>By staying on an underpowered compiler, you are putting your head in the sand and crossing fingers that no vulnerability is found. If your product is a high-value target, I urge you to reconsider.</p>
<hr/>
<p><em>Note: even if you do NOT upgrade the production compiler, you might want to use a new compiler to check for vulnerability anyway; do be aware that since those are different compilers, the guarantees are lessened though.</em></p>
</div></text><author><a href="/users/147192/matthieu-m">Matthieu M.</a></author><comments><comment><text><span class="comment-copy">+1 for bothering to mention cases in which new compilers can be more secure, rather than piling on the 'b-but my good old UB' bandwagon of the other answers. this is on top of the many other improvements they offer that aren't directly related to security but provide even more impetus to be reasonable modern.</span></text><author><a class="comment-user" href="/users/2757035/underscore-d" title="2,022 reputation">underscore_d</a></author></comment><comment><text><span class="comment-copy">Though it feels like advocating 'security through obscurity'; the bugs that affect old compilers are known and public. While I agree that new compilers will introduce bugs, these bugs are not yet public like those for previous versions are, which is some security if you're updating the application often.</span></text><author><a class="comment-user" href="/users/3492369/the6p4c" title="514 reputation">The6P4C</a></author></comment><comment><text><span class="comment-copy">Chandler Carruth is so cute, and speaks of such wonderful things. I'd marry him if I could.</span></text><author><a class="comment-user" href="/users/1155000/daniel-kamil-kozar" title="9,576 reputation">Daniel Kamil Kozar</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>Your compiled code contains bugs that could be exploited. The bugs come from three sources: Bugs in your source code, bugs in the compiler and libraries, and undefined behaviour in your source code that the compiler turns into a bug. (Undefined behaviour is a bug, but not a bug in the compiled code yet. As an example, i = i++; in C or C++ is a bug, but in your compiled code it may increase i by 1 and be Ok, or set i to some junk and be a bug). </p>
<p>The rate of bugs in your compiled code is presumably low due to testing and to fixing bugs due to customer bug reports. So there may have been a large number of bugs initially, but that has gone down. </p>
<p>If you upgrade to a newer compiler, you may lose bugs that were introduced by compiler bugs. But these bugs would all be bugs that to your knowledge nobody found and nobody exploited. But the new compiler may have bugs on its own, and importantly newer compilers have a stronger tendency to turn undefined behaviour into bugs in the compiled code. </p>
<p>So you will have a whole lot of new bugs in your compiled code; all bugs that hackers could find and exploit. And unless you do a whole lot of testing, and leave your code with customers to find bugs for a long time, it will be less secure. </p>
</div></text><author><a href="/users/3255455/gnasher729">gnasher729</a></author><comments><comment><text><span class="comment-copy">So in other words... there's no easy way to tell what problems the compiler introduces, and by switching all you do is get a different set of unknown issues?</span></text><author><a class="comment-user" href="/users/6352535/jeremy-kato" title="428 reputation">Jeremy Kato</a></author></comment><comment><text><span class="comment-copy">@JeremyKato: well, there are some cases where you're <i>also</i> getting a different set of known issues. I'm not sure what known security flaws there are in the compiler itself, but for the sake of a concrete example suppose that updating to a new compiler means also being able to take the latest libc (while using the old one means not being able to do this), then you'd <i>know</i> you're fixing this flaw in <code>getaddrinfo()</code>: <a href="https://access.redhat.com/articles/2161461" rel="nofollow noreferrer">access.redhat.com/articles/2161461</a> . That example's not actually a compiler security flaw, but over 10+ years there are bound to be some known fixed flaws.</span></text><author><a class="comment-user" href="/users/13005/steve-jessop" title="211,507 reputation">Steve Jessop</a></author></comment><comment><text><span class="comment-copy">Heh, actually that flaw was only introduced in 2008 so the questioner might be safe from it. But my point is not about that particular example, it's that there do exist known bugs that an old toolchain will put into your code. So when you update it's true that you introduce a new set of unknowns, but that's not <i>all you do</i>. You basically just have to guess whether you're "more secure" leaving in the known critical flaw that the newest toolchain fixes, or taking the unknown consequences of rolling the dice again on all the undefined behaviour in your own code.</span></text><author><a class="comment-user" href="/users/13005/steve-jessop" title="211,507 reputation">Steve Jessop</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p><em>If it aint broke, don't fix it</em></p>
<p>Your boss sounds right in saying this, however, the more <strong>important</strong> factor, is safeguarding of inputs, outputs, buffer overflows. Lack of those is invariably the weakest link in the chain from that standpoint regardless of the compiler used.</p>
<p>However, if the code base is ancient, and work was put in place to mitigate the weaknesses of the K&amp;R C used, such as lacking of type safety, insecure fgets, etc, weigh up the question "<em>Would upgrading the compiler to more modern C99/C11 standards break everything?</em>"</p>
<p>Provided, that there's a clear path to migrate to the newer C standards, which could induce side effects, might be best to attempt a fork of the old codebase, assess it and put in extra type checks, sanity checks, and determine if upgrading to the newer compiler has any effect on input/output datasets. </p>
<p>Then you can show it to your boss, "<em>Here's the updated code base, refactored, more in line with industry accepted C99/C11 standards...</em>".</p>
<p>That's the gamble that would have to be weighed up on, <em>very carefully</em>, <em>resistence to change</em> might show there in that environment and may refuse to touch the newer stuff.</p>
<p><strong>EDIT</strong></p>
<p>Just sat back for a few minutes, realized this much, K&amp;R generated code could be running on a 16bit platform, chances are, upgrading to more modern compiler could actually break the code base, am thinking in terms of architecture, 32bit code would be generated, this could have funny side effects on the structures used for input/output datasets, that is another <strong>huge</strong> factor to weigh up carefully. </p>
<p>Also, since OP has mentioned using Visual Studio 2008 to build the codebase, using gcc could induce bringing into the environment either MinGW or Cygwin, that could have an impact change on the environment, unless, the target is for Linux, then it would be worth a shot, may have to include additional switches to the compiler to minimize noise on old K&amp;R code base, the other important thing is to carry out a lot of testing to ensure no functionality is broken, may turn out to be a painful exercise. </p>
</div></text><author><a href="/users/206367/t0mm13b">t0mm13b</a></author><comments><comment><text><span class="comment-copy">The same code is built by Visual Studio 2008 for Windows targets, and MSVC doesn't support C99 or C11 yet (I don't know if newer MSVC does), and I can build it on my Linux box using the latest GCC. So if we would just drop in a newer GCC it would probably build just as fine as before.</span></text><author><a class="comment-user owner" href="/users/58805/calmarius" title="6,593 reputation">Calmarius</a></author></comment><comment><text><span class="comment-copy">@Calmarius thanks for the heads up, maybe it might be best to edit your question to include the comment, That's important :) And should have been there ;D</span></text><author><a class="comment-user" href="/users/206367/t0mm13b" title="27,001 reputation">t0mm13b</a></author></comment><comment><text><span class="comment-copy">@Calmarius have edited my answer, which is my thinking on the newly updated question.</span></text><author><a class="comment-user" href="/users/206367/t0mm13b" title="27,001 reputation">t0mm13b</a></author></comment><comment><text><span class="comment-copy">“could be running on a 16bit platform, chances are, upgrading to more modern compiler could actually break the code base, am thinking in terms of architecture, 32bit code” I don't think the question is about <b>porting</b> code to new implementation-defined parameters.</span></text><author><a class="comment-user" href="/users/139746/pascal-cuoq" title="61,236 reputation">Pascal Cuoq</a></author></comment><comment><text><span class="comment-copy">Agreed. It's <i>possible</i> that a runtime vulnerability could be created by a compiler bug. But it's <i>far more likely</i> that the code contains runtime vulnerabilities due to such things as buffer and stack overruns. So, when you invest time in making this code base more secure, you should invest it in doing things like checking the length of input strings to ensure they don't exceed your program's limits. Getting a newer compiler won't help much. Rewriting the code from scratch in a language with native string and array objects will help a lot. But your boss won't pay for that.</span></text><author><a class="comment-user" href="/users/205608/o-jones" title="44,869 reputation">O. Jones</a></author></comment><comment><text><span class="comment-copy">And, VS2008's C compiler was pretty darn good, as were the various GCC compilers.</span></text><author><a class="comment-user" href="/users/205608/o-jones" title="44,869 reputation">O. Jones</a></author></comment><comment><text><span class="comment-copy">In the ’80s it may have been K&amp;R, but by now, with the given compilers, I presume it is ANSI C. That is already a considerable improvement.</span></text><author><a class="comment-user" href="/users/4847772/pjtraill" title="900 reputation">PJTraill</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<blockquote>
<p>Can using an old C compiler compromise the security of the compiled program? </p>
</blockquote>
<p>Of course it can, if the old compiler contains known bugs that you know would affect your program. </p>
<p>The question is, does it? To know for sure, you would have to read the whole change log from your version to present date and check every single bug fixed over the years.</p>
<p>If you find no evidence of compiler bugs that would affect your program, updating GCC just for the sake of it seems a bit paranoid. You would have to keep in mind that newer versions might contain new bugs, that are not yet discovered. Lots of changes were made recently with GCC 5 and C11 support.</p>
<p>That being said, code written in the 80s is most likely already filled to the brim with security holes and reliance on poorly-defined behavior, no matter the compiler. We're talking of pre-standard C here.</p>
</div></text><author><a href="/users/584518/lundin">Lundin</a></author><comments><comment><text><span class="comment-copy">I don't think it's paranoia; I think the OP is trying to invent reasons to convince his boss.  Probably the OP actually wants a new compiler because they make better asm (including cross-file optimization with LTO), have more useful diagnostics/warnings, and allow modern language features and syntax.   (e.g. C11 stdatomic).</span></text><author><a class="comment-user" href="/users/224132/peter-cordes" title="54,745 reputation">Peter Cordes</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>There is a security risk where a malicious developer can sneak a back-door through a compiler bug. Depending on the quantity of known bugs in the compiler in use, the backdoor may look more or less inconspicuous (in any case, the point is that the code is correct, even if convoluted, at the source level. Source code reviews and tests using a non-buggy compiler will not find the backdoor, because the backdoor does not exist in these conditions). For extra deniability points, the malicious developer may also look for previously-unknown compiler bugs on their own. Again, the quality of the camouflage will depend on the choice of compiler bugs found.</p>
<p>This attack is illustrated on the program sudo in <a href="https://www.alchemistowl.org/pocorgtfo/pocorgtfo08.pdf#page=7" rel="nofollow">this article</a>. bcrypt wrote a great follow-up for <a href="https://zyan.scripts.mit.edu/blog/backdooring-js/" rel="nofollow">Javascript minifiers</a>.</p>
<p>Apart from this concern, the evolution of C compilers has been to exploit undefined behavior <a href="https://godbolt.org/g/wVEoTM" rel="nofollow">more</a> and <a href="https://lwn.net/Articles/342330/" rel="nofollow">more</a> and <a href="https://lwn.net/Articles/278137/" rel="nofollow">more</a> aggressively, so old C code that was written in good faith would actually be more secure compiled with a C compiler from the time, or compiled at -O0 (but some new program-breaking UB-exploiting optimizations <a href="http://blog.mycre.ws/articles/bind-and-gcc-49/" rel="nofollow">are introduced in new versions of compilers even at -O0</a>).</p>
</div></text><author><a href="/users/139746/pascal-cuoq">Pascal Cuoq</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>Older compilers may not have protection against known hacking attacks. Stack smashing protection, for example, was not introduced <a href="https://en.wikipedia.org/wiki/Buffer_overflow_protection#GNU_Compiler_Collection_.28GCC.29">until GCC 4.1</a>. So yeah, code compiled with older compilers may be vulnerable in ways that newer compilers protect against.</p>
</div></text><author><a href="/users/5609344/drmccleod">DrMcCleod</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>Another aspect to worry about is the <em>development of new code</em>.</p>
<p>Older compilers may have different behavior for some language features than what is standardized and expected by the programmer. This mismatch can slow development and introduce subtle bugs that can be exploited.</p>
<p>Older compilers offer fewer features (including language features!) and don't optimize as well. Programmers will hack their way around these deficiencies — e.g. by reimplementing missing features, or writing clever code that is obscure but runs faster — creating new opportunities for the creation of subtle bugs.</p>
</div></text><author><a href="/users/1084944/hurkyl">Hurkyl</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<h2>Nope</h2>
<p>The reason is simple, old compiler may have old bugs and exploits, but the new compiler will have new bugs and exploits. </p>
<p>Your not "fixing" any bugs by upgrading to a new compiler. Your switching old bugs and exploits for new bugs and exploits. </p>
</div></text><author><a href="/users/1794978/coteyr">coteyr</a></author><comments><comment><text><span class="comment-copy">These seems very simplistic: the new compiler may have its weaknesses, but I would expect them to be less than in the old compiler, and it is likely to detect several code vulnerabilities that have become known since then.</span></text><author><a class="comment-user" href="/users/4847772/pjtraill" title="900 reputation">PJTraill</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>Well there is a higher probability that any bugs in the old compiler are well known and documented as opposed to using a new compiler so actions can be taken to avoid those bugs by coding around them. So in a way that is not enough as argument for upgrading. We have the same discussions where I work, we use GCC 4.6.1 on a code base for embedded software and there is a great reluctance (among management) to upgrade to the latest compiler because of fear for new, undocumented bugs. </p>
</div></text><author><a href="/users/45685/anders-k">Anders K.</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>Your question falls into two parts:</p>
<ul>
<li>Explicit: “Is there a greater risk in using the older compiler” (more or less as in your title)</li>
<li>Implicit: “How can I persuade management to upgrade”</li>
</ul>
<p>Perhaps you can answer both by finding an exploitable flaw in your existing code base and showing that a newer compiler would have detected it. Of course your management may say “you found that with the old compiler”, but you can point out that it cost considerable effort. Or you run it through the new compiler to find the vulnerability, then exploit it, if your are able/allowed to compile the code with the new compiler. You may want help from a friendly hacker, but that depends on trusting them and being able/allowed to show them the code (and use the new compiler).</p>
<p>But if your system is not exposed to hackers, you should perhaps be more interested in whether a compiler upgrade would increase your effectiveness: MSVS 2013 Code Analysis quite often finds potential bugs much sooner than MSVS 2010, and it more or less supports C99/C11 – not sure if it does officially, but declarations can follow statements and you can declare variables in <code>for</code>-loops.</p>
</div></text><author><a href="/users/4847772/pjtraill">PJTraill</a></author><comments/></answer></answers></post>