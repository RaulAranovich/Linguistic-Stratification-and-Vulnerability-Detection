<?xml version="1.0" encoding="utf-8"?>
<post><title>security - Is "double hashing" a password less secure than just hashing it once? - Stack Overflow</title><question><text><div class="post-text" itemprop="text">
<p>Is hashing a password twice before storage any more or less secure than just hashing it once?</p>
<p>What I'm talking about is doing this:</p>
<pre><code>$hashed_password = hash(hash($plaintext_password));
</code></pre>
<p>instead of just this:</p>
<pre><code>$hashed_password = hash($plaintext_password);
</code></pre>
<p>If it is less secure, can you provide a good explanation (or a link to one)?</p>
<p>Also, does the hash function used make a difference?  Does it make any difference if you mix md5 and sha1 (for example) instead of repeating the same hash function?</p>
<p>Note 1:  When I say "double hashing" I'm talking about hashing a password twice in an attempt to make it more obscured.  I'm not talking about the <a href="http://en.wikipedia.org/wiki/Double_hashing" rel="noreferrer">technique for resolving collisions</a>.</p>
<p><strong>Note 2:  I know I need to add a random salt to really make it secure.  The question is whether hashing twice with the same algorithm helps or hurts the hash.</strong></p>
</div></text><author><a href="/users/1350209/cole-johnson">Cole Johnson</a></author><comments><comment><text><span class="comment-copy"><code>Hash(password)</code> and <code>Hash(Hash(password))</code> are equally insecure. Both lack the notion of <a href="http://en.wikipedia.org/wiki/Semantic_security" rel="nofollow noreferrer">Semantic Security</a>. That is, the output <i>is</i> distinguishable from random. For example, <code>MD5("password")</code> is <code>5f4dcc3b5aa765d61d8327deb882cf99</code>. I know that's the MD5 hash of <code>password</code>, and it <b><i>is</i></b> distinguishable from random. Instead, you should use an HMAC. Its provably secure and its a PRF.</span></text><author><a class="comment-user" href="/users/608639/jww" title="41,097 reputation">jww</a></author></comment></comments></question><answers><answer><text><div class="post-text" itemprop="text">
<h2>Hashing a password once is insecure</h2>
<p>No, multiple hashes are not less secure; they are an essential part of secure password use.</p>
<p>Iterating the hash increases the time it takes for an attacker to try each password in their list of candidates. You can easily increase the time it takes to attack a password from hours to years.</p>
<h2>Simple iteration is not enough</h2>
<p>Merely chaining hash output to input isn't sufficient for security. The iteration should take place in the context of an algorithm that preserves the entropy of the password. Luckily, there are several published algorithms that have had enough scrutiny to give confidence in their design.</p>
<p>A good key derivation algorithm like PBKDF2 injects the password into each round of hashing, mitigating concerns about collisions in hash output. PBKDF2 can be used for password authentication as-is. Bcrypt follows the key derivation with an encryption step; that way, if a fast way to reverse the key derivation is discovered, an attacker still has to complete a known-plaintext attack.</p>
<h2>How to break a password</h2>
<p>Stored passwords need protection from an offline attack. If passwords aren't salted, they can be broken with a pre-computed dictionary attack (for example, using a Rainbow Table). Otherwise, the attacker must spend time to compute a hash for each password and see if it matches the stored hash.</p>
<p>All passwords are not equally likely. Attackers might exhaustively search all short passwords, but they know that their chances for brute-force success drop sharply with each additional character. Instead, they use an ordered list of the most likely passwords. They start with "password123" and progress to less frequently used passwords. </p>
<p>Let's say an attackers list is long, with 10 billion candidates; suppose also that a desktop system can compute 1 million hashes per second. The attacker can test her whole list is less than three hours if only one iteration is used. But if just 2000 iterations are used, that time extends to almost 8 months. To defeat a more sophisticated attacker—one capable of downloading a program that can tap the power of their GPU, for example—you need more iterations.</p>
<h2>How much is enough?</h2>
<p>The number of iterations to use is a trade-off between security and user experience. Specialized hardware that can be used by attackers is cheap, but <a href="https://en.bitcoin.it/wiki/Mining_hardware_comparison" rel="noreferrer">it can still perform hundreds of millions of iterations per second.</a> The performance of the <em>attacker's</em> system determines how long it takes to break a password given a number of iterations. But your application is not likely to use this specialized hardware. How many iterations you can perform without aggravating users depends on <em>your</em> system. </p>
<p>You can probably let users wait an extra ¾ second or so during authentication. Profile your target platform, and use as many iterations as you can afford. Platforms I've tested (one user on a mobile device, or many users on a server platform) can comfortably support <a href="ftp://ftp.rsasecurity.com/pub/pkcs/pkcs-5v2/pkcs5v2_1.pdf" rel="noreferrer">PBKDF2</a> with between 60,000 and 120,000 iterations, or <a href="http://www.usenix.org/events/usenix99/provos.html" rel="noreferrer">bcrypt</a> with cost factor of 12 or 13.</p>
<h2>More background</h2>
<p>Read PKCS #5 for authoritative information on the role of salt and iterations in hashing. Even though PBKDF2 was meant for generating encryption keys from passwords, it works well as a one-way-hash for password authentication. Each iteration of bcrypt is more expensive than a SHA-2 hash, so you can use fewer iterations, but the idea is the same. Bcrypt also goes a step beyond most PBKDF2-based solutions by using the derived key to encrypt a well-known plain text. The resulting cipher text is stored as the "hash," along with some meta-data. However, nothing stops you from doing the same thing with PBKDF2.</p>
<p>Here are other answers I've written on this topic:</p>
<ul>
<li><a href="http://stackoverflow.com/questions/312088/what-is-the-easiest-way-to-create-and-compare-a-salted-password-in-net#312159">Hashing passwords</a></li>
<li><a href="http://stackoverflow.com/questions/287517/encryptinghashing-plain-text-passwords-in-database#287738">Hashing passwords</a></li>
<li><a href="http://stackoverflow.com/questions/55862/how-to-implement-password-protection-for-individual-files#55904">Salt</a></li>
<li><a href="http://stackoverflow.com/questions/213380/the-necessity-of-hiding-the-salt-for-a-hash#215165">Hiding salt</a></li>
<li><a href="http://stackoverflow.com/a/1561245/3474">PBKDF2 versus bcrypt</a></li>
<li><a href="http://stackoverflow.com/a/6833165/3474">Bcrypt</a></li>
</ul>
</div></text><author><a href="/users/3474/erickson">erickson</a></author><comments><comment><text><span class="comment-copy">It is less secure mathematically, and you'd be much better off using a sleep() than intentionally making a slow algorithm</span></text><author><a class="comment-user" href="/users/24181/greg" title="214,936 reputation">Greg</a></author></comment><comment><text><span class="comment-copy">Number of arbitrary strings: infinity. number of possible MD5 values: 2^128. Sleep works because they are using your server to authenticate in RoBorg's scenario. Now, number of likely passwords is much lower than 2^128, and so is number of derived Md5s, so that argument is questionable.</span></text><author><a class="comment-user" href="/users/15962/squarecog" title="15,914 reputation">SquareCog</a></author></comment><comment><text><span class="comment-copy">Intentionally making a slow algorithm is an accepted practice when you're trying to prevent dictionary attacks against compromised authentication stores. The technique is called "key strengthening" or "key stretching". See <a href="http://en.wikipedia.org/wiki/Key_stretching" rel="nofollow noreferrer">en.wikipedia.org/wiki/Key_stretching</a></span></text><author><a class="comment-user" href="/users/41871/willie-wheeler" title="16,772 reputation">Willie Wheeler</a></author></comment><comment><text><span class="comment-copy">@RoBorg: it doesn't matter how slow <i>your</i> implementation is, but how slow an attacker's implementation will be: if the hash itself is thousands of times slower, it will take an attacker thousands of times as long to brute-force the password.</span></text><author><a class="comment-user" href="/users/37020/orip" title="40,336 reputation">orip</a></author></comment><comment><text><span class="comment-copy">@erickson - with PBKDF2 you increase the number of iterations to increase the processing time, so that isn't a special bcrypt property. Bcrypt, on the other hand, hasn't received nearly as much cryptological review as HMAC and the SHA-2 hashes have.</span></text><author><a class="comment-user" href="/users/37020/orip" title="40,336 reputation">orip</a></author></comment><comment><text><span class="comment-copy">Ahh I see what you mean now</span></text><author><a class="comment-user" href="/users/24181/greg" title="214,936 reputation">Greg</a></author></comment><comment><text><span class="comment-copy">Arguably you would want collisions within the 128-bit space 0 through 2^128-1. If the hash algorithm's 2^128 output space is perfect, then theoretically, you just have a substitution cipher with an alphabet of 2^128 glyphs.</span></text><author><a class="comment-user" href="/users/44065/jmucchiello" title="13,247 reputation">jmucchiello</a></author></comment><comment><text><span class="comment-copy">Substitution ciphers are weak because frequency distributions in the plaintext are propagated to the ciphertext. That wouldn't apply here, where every plaintext "character" is equally likely.</span></text><author><a class="comment-user" href="/users/3474/erickson" title="191,723 reputation">erickson</a></author></comment><comment><text><span class="comment-copy">The chances of you coming across an md5 collision by accident is the same as winning the intergalactic lottery, its just not going to happen. <a href="http://stackoverflow.com/questions/800685/which-hash-function-should-i-choose/817121#817121" title="which hash function should i choose">stackoverflow.com/questions/800685/…</a></span></text><author><a class="comment-user" href="/users/17174/sam-saffron" title="78,766 reputation">Sam Saffron</a></author></comment><comment><text><span class="comment-copy">Wouldn't it be true that if MD5(K) = [X byte result] then MD5(MD5(K)) is LESS secure (more prone to collision) when your K is &gt; [X bytes], and equally secure (i.e. redundant/pointless) when K =&lt; [X Bytes]?  Restricting the amount of times a user can retry a password is a CRITICAL security measure, but if you are increasing your CPU usage 1000 fold for every user, then you solution is ATROCIOUSLY unscalable.</span></text><author><a class="comment-user" href="/users/59808/devinb" title="6,538 reputation">DevinB</a></author></comment><comment><text><span class="comment-copy">So, (because I didn't explicitly say this) you should shouldn't be hashing twice, you should favour other methods that are specifically designed to limit the number of retries, rather than taxing the CPU to do it for you.</span></text><author><a class="comment-user" href="/users/59808/devinb" title="6,538 reputation">DevinB</a></author></comment><comment><text><span class="comment-copy">@devin -- it's not "my solution", it's a widely accepted practice, built into password-based cryptography standards like PKCS #5, and recommended by experts like Robert Morris. It's extremely scalable, as the fraction of time spent authenticating users is small in a legitimate application. It only becomes difficult to scale when your application is cracking passwords—hence the recommendation. Certainly, the search space of a hash is smaller than that of possible passwords, but even a 128-bit space is too huge to brute-force search. The threat to defend against is an offline dictionary attack.</span></text><author><a class="comment-user" href="/users/3474/erickson" title="191,723 reputation">erickson</a></author></comment><comment><text><span class="comment-copy">I was referring not to the inconvenience to the individual user, but rather the stress that would be put upon the server if you had a large user base, because you are relying on the CPU load to slow down the number of requests. It means that if you add more CPU power, you are reducing the restriction on those brute force attackers. -- However, you are completely correct about the scalability, and the widely accepted practice. I was wrong about almost all the things I said in my earlier comments. Sorry :)</span></text><author><a class="comment-user" href="/users/59808/devinb" title="6,538 reputation">DevinB</a></author></comment><comment><text><span class="comment-copy">For reference, salt is meant to defend against rainbow tables, not dictionary attacks.  There's actually very little you can do to defend against dictionary attacks except (1) put some constraints on the password before letting a user set it (for example, "must not appear in my list of common words"), and/or (2) make the hashed password take a very long time to compute, which is the point of multiple rounds of hashing.</span></text><author><a class="comment-user" href="/users/319403/chao" title="58,174 reputation">cHao</a></author></comment><comment><text><span class="comment-copy">@cHao: I meant pre-computed dictionary, in the sense of an indexed table. "Rainbow table" is just one specific implementation of this general class of attack.</span></text><author><a class="comment-user" href="/users/3474/erickson" title="191,723 reputation">erickson</a></author></comment><comment><text><span class="comment-copy">Note that PBKDF2 re-input the password and salt in each step to avoid losing entropy through repeated hashes. MD5 is most likely not injective on the 128-bit input space (i.e. has collisions and some output values which are not reached) - hash functions model random functions, not random permutations.</span></text><author><a class="comment-user" href="/users/600500/pa%c5%adlo-ebermann" title="51,688 reputation">Paŭlo Ebermann</a></author></comment><comment><text><span class="comment-copy">Ultimately much of this doesn't matter for the question, since he shouldn't be using MD5. Should be using some higher SHA or BCrypt</span></text><author><a class="comment-user" href="/users/881421/dftr" title="509 reputation">DFTR</a></author></comment><comment><text><span class="comment-copy">Wrong, bad, and dangerous. <b>Absolutely do not ever</b> chain multiple iterations of a conventional hash function in this way. Use a password hash instead.</span></text><author><a class="comment-user" href="/users/152948/hobbs" title="116,879 reputation">hobbs</a></author></comment><comment><text><span class="comment-copy">@hobbs You mean like PBKDF2 or bcrypt?</span></text><author><a class="comment-user" href="/users/3474/erickson" title="191,723 reputation">erickson</a></author></comment><comment><text><span class="comment-copy">Yes, and now we're less bad and wrong than 15 minutes ago ;)</span></text><author><a class="comment-user" href="/users/152948/hobbs" title="116,879 reputation">hobbs</a></author></comment><comment><text><span class="comment-copy">@hobbs bcrypt and PBKDF2 were always in there; hopefully they are more prominent now. The point of this answer is that hashing once is stupid.</span></text><author><a class="comment-user" href="/users/3474/erickson" title="191,723 reputation">erickson</a></author></comment><comment><text><span class="comment-copy">@erickson and with that, I agree. :)</span></text><author><a class="comment-user" href="/users/152948/hobbs" title="116,879 reputation">hobbs</a></author></comment><comment><text><span class="comment-copy">@hobbs The difference between an iterated hash and PBKDF2 is negligible. You lose entropy, but it's easy to show that for an ideal hash that loss is quite small. So your language against simple iterated hashes is too strong. It's not best-practice, but it isn't a practical concern either.</span></text><author><a class="comment-user" href="/users/445517/codesinchaos" title="78,763 reputation">CodesInChaos</a></author></comment><comment><text><span class="comment-copy">@CodesInChaos, "You lose entropy..." Just to make sure I understand correctly: do we lose entropy during the iterations because the hash functions are (known to be) not perfect? Thanks!</span></text><author><a class="comment-user" href="/users/1479945/sz" title="827 reputation">Sz.</a></author></comment><comment><text><span class="comment-copy">@lunakid A hash function is not a permutation. So even for ideal hashes there are collisions which cost entropy.</span></text><author><a class="comment-user" href="/users/445517/codesinchaos" title="78,763 reputation">CodesInChaos</a></author></comment><comment><text><span class="comment-copy">@CodesInChaos, I'm not sure what you mean by "ideal hash", but a "perfect hash" (<a href="http://en.wikipedia.org/wiki/Perfect_hash_function" rel="nofollow noreferrer">en.wikipedia.org/wiki/Perfect_hash_function</a>) maps an input set S onto the same S, and <i>is</i> a permutation (I think), with <i>no</i> collisions. What I was wondering about: can a cryptographic hash (which is <i>not</i> perfect, as it maps "infinity" onto S, to begin with) behave as "perfect", when its input set is narrowed to be the same as its output -- which is the case with repeated hashing --, or that is impossible (which needs some proof then, at least I'm not smart enough to just "see" that).</span></text><author><a class="comment-user" href="/users/1479945/sz" title="827 reputation">Sz.</a></author></comment><comment><text><span class="comment-copy">@lunakid If your function is bijective, it's a permutation and violates the properties we require out of a cryptographic hash. So we don't expect commonly used hashes to be bijective, see <a href="http://crypto.stackexchange.com/questions/301/is-sha-512-bijective-when-hashing-a-single-512-bit-block">Is SHA-512 bijective when hashing a single 512-bit block? on crypto.SE</a> for one variant of this.</span></text><author><a class="comment-user" href="/users/445517/codesinchaos" title="78,763 reputation">CodesInChaos</a></author></comment><comment><text><span class="comment-copy">@CodesInChaos: Great, now it's getting clearer, thanks! :)</span></text><author><a class="comment-user" href="/users/1479945/sz" title="827 reputation">Sz.</a></author></comment><comment><text><span class="comment-copy">Is it assumed in all of this that the attacker knows the hashing algorithm?  Is it true that repeating a hashing function n times makes things more difficult for the attacker if they don't know the value of n?</span></text><author><a class="comment-user" href="/users/1319904/daniel-howard" title="1,312 reputation">Daniel Howard</a></author></comment><comment><text><span class="comment-copy">@DanielHoward Yes, assume the attacker knows everything except the password. Otherwise, you overestimate the security. Not knowing the value of <code>n</code> is not a significant obstacle for the attacker; they just keep iterating until they get a match. It's like a <code>while</code> loop instead of a <code>for</code> loop.</span></text><author><a class="comment-user" href="/users/3474/erickson" title="191,723 reputation">erickson</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>To those who say it's secure, they are correct <strong>in general</strong>. "Double" hashing (or the logical expansion of that, iterating a hash function) is absolutely secure <strong>if done right</strong>, for a specific concern.</p>
<p>To those who say it's insecure, they are correct <strong>in this case</strong>. The code that is posted in the question <strong>is</strong> insecure. Let's talk about why:</p>
<pre><code>$hashed_password1 = md5( md5( plaintext_password ) );
$hashed_password2 = md5( plaintext_password );
</code></pre>
<p>There are two fundamental properties of a hash function that we're concerned about:</p>
<ol>
<li><p><em>Pre-Image Resistance</em> - Given a hash <code>$h</code>, it should be difficult to find a message <code>$m</code> such that <code>$h === hash($m)</code></p></li>
<li><p><em>Second-Pre-Image Resistance</em> - Given a message <code>$m1</code>, it should be difficult to find a different message <code>$m2</code> such that <code>hash($m1) === hash($m2)</code></p></li>
<li><p><em>Collision Resistance</em> - It should be difficult to find a pair of messages <code>($m1, $m2)</code> such that <code>hash($m1) === hash($m2)</code> (note that this is similar to Second-Pre-Image resistance, but different in that here the attacker has control over both messages)...</p></li>
</ol>
<p><strong>For the storage of passwords</strong>, all we really care about is <em>Pre-Image Resistance</em>. The other two would be moot, because <code>$m1</code> is the user's password we're trying to keep safe. So if the attacker already has it, the hash has nothing to protect...</p>
<h1>DISCLAIMER</h1>
<p>Everything that follows is based on the premise that all we care about is <em>Pre-Image Resistance</em>. The other two fundamental properties of hash functions may not (and typically don't) hold up in the same way. So the conclusions in this post are <strong>only applicable when using hash functions for the storage of passwords. They are not applicable in general...</strong></p>
<h1>Let's Get Started</h1>
<p>For the sake of this discussion, let's invent our own hash function:</p>
<pre><code>function ourHash($input) {
    $result = 0;
    for ($i = 0; $i &lt; strlen($input); $i++) {
        $result += ord($input[$i]);
    }
    return (string) ($result % 256);
}
</code></pre>
<p>Now it should be pretty obvious what this hash function does. It sums together the ASCII values of each character of input, and then takes the modulo of that result with 256. </p>
<p>So let's test it out:</p>
<pre><code>var_dump(
    ourHash('abc'), // string(2) "38"
    ourHash('def'), // string(2) "47"
    ourHash('hij'), // string(2) "59"
    ourHash('klm')  // string(2) "68"
);
</code></pre>
<p>Now, let's see what happens if we run it a few times around a function:</p>
<pre><code>$tests = array(
    "abc",
    "def",
    "hij",
    "klm",
);

foreach ($tests as $test) {
    $hash = $test;
    for ($i = 0; $i &lt; 100; $i++) {
        $hash = ourHash($hash);
    }
    echo "Hashing $test =&gt; $hash\n";
}
</code></pre>
<p>That outputs:</p>
<pre><code>Hashing abc =&gt; 152
Hashing def =&gt; 152
Hashing hij =&gt; 155
Hashing klm =&gt; 155
</code></pre>
<p>Hrm, wow. We've generated collisions!!! Let's try to look at why:</p>
<p>Here's the output of hashing a string of each and every possible hash output:</p>
<pre><code>Hashing 0 =&gt; 48
Hashing 1 =&gt; 49
Hashing 2 =&gt; 50
Hashing 3 =&gt; 51
Hashing 4 =&gt; 52
Hashing 5 =&gt; 53
Hashing 6 =&gt; 54
Hashing 7 =&gt; 55
Hashing 8 =&gt; 56
Hashing 9 =&gt; 57
Hashing 10 =&gt; 97
Hashing 11 =&gt; 98
Hashing 12 =&gt; 99
Hashing 13 =&gt; 100
Hashing 14 =&gt; 101
Hashing 15 =&gt; 102
Hashing 16 =&gt; 103
Hashing 17 =&gt; 104
Hashing 18 =&gt; 105
Hashing 19 =&gt; 106
Hashing 20 =&gt; 98
Hashing 21 =&gt; 99
Hashing 22 =&gt; 100
Hashing 23 =&gt; 101
Hashing 24 =&gt; 102
Hashing 25 =&gt; 103
Hashing 26 =&gt; 104
Hashing 27 =&gt; 105
Hashing 28 =&gt; 106
Hashing 29 =&gt; 107
Hashing 30 =&gt; 99
Hashing 31 =&gt; 100
Hashing 32 =&gt; 101
Hashing 33 =&gt; 102
Hashing 34 =&gt; 103
Hashing 35 =&gt; 104
Hashing 36 =&gt; 105
Hashing 37 =&gt; 106
Hashing 38 =&gt; 107
Hashing 39 =&gt; 108
Hashing 40 =&gt; 100
Hashing 41 =&gt; 101
Hashing 42 =&gt; 102
Hashing 43 =&gt; 103
Hashing 44 =&gt; 104
Hashing 45 =&gt; 105
Hashing 46 =&gt; 106
Hashing 47 =&gt; 107
Hashing 48 =&gt; 108
Hashing 49 =&gt; 109
Hashing 50 =&gt; 101
Hashing 51 =&gt; 102
Hashing 52 =&gt; 103
Hashing 53 =&gt; 104
Hashing 54 =&gt; 105
Hashing 55 =&gt; 106
Hashing 56 =&gt; 107
Hashing 57 =&gt; 108
Hashing 58 =&gt; 109
Hashing 59 =&gt; 110
Hashing 60 =&gt; 102
Hashing 61 =&gt; 103
Hashing 62 =&gt; 104
Hashing 63 =&gt; 105
Hashing 64 =&gt; 106
Hashing 65 =&gt; 107
Hashing 66 =&gt; 108
Hashing 67 =&gt; 109
Hashing 68 =&gt; 110
Hashing 69 =&gt; 111
Hashing 70 =&gt; 103
Hashing 71 =&gt; 104
Hashing 72 =&gt; 105
Hashing 73 =&gt; 106
Hashing 74 =&gt; 107
Hashing 75 =&gt; 108
Hashing 76 =&gt; 109
Hashing 77 =&gt; 110
Hashing 78 =&gt; 111
Hashing 79 =&gt; 112
Hashing 80 =&gt; 104
Hashing 81 =&gt; 105
Hashing 82 =&gt; 106
Hashing 83 =&gt; 107
Hashing 84 =&gt; 108
Hashing 85 =&gt; 109
Hashing 86 =&gt; 110
Hashing 87 =&gt; 111
Hashing 88 =&gt; 112
Hashing 89 =&gt; 113
Hashing 90 =&gt; 105
Hashing 91 =&gt; 106
Hashing 92 =&gt; 107
Hashing 93 =&gt; 108
Hashing 94 =&gt; 109
Hashing 95 =&gt; 110
Hashing 96 =&gt; 111
Hashing 97 =&gt; 112
Hashing 98 =&gt; 113
Hashing 99 =&gt; 114
Hashing 100 =&gt; 145
Hashing 101 =&gt; 146
Hashing 102 =&gt; 147
Hashing 103 =&gt; 148
Hashing 104 =&gt; 149
Hashing 105 =&gt; 150
Hashing 106 =&gt; 151
Hashing 107 =&gt; 152
Hashing 108 =&gt; 153
Hashing 109 =&gt; 154
Hashing 110 =&gt; 146
Hashing 111 =&gt; 147
Hashing 112 =&gt; 148
Hashing 113 =&gt; 149
Hashing 114 =&gt; 150
Hashing 115 =&gt; 151
Hashing 116 =&gt; 152
Hashing 117 =&gt; 153
Hashing 118 =&gt; 154
Hashing 119 =&gt; 155
Hashing 120 =&gt; 147
Hashing 121 =&gt; 148
Hashing 122 =&gt; 149
Hashing 123 =&gt; 150
Hashing 124 =&gt; 151
Hashing 125 =&gt; 152
Hashing 126 =&gt; 153
Hashing 127 =&gt; 154
Hashing 128 =&gt; 155
Hashing 129 =&gt; 156
Hashing 130 =&gt; 148
Hashing 131 =&gt; 149
Hashing 132 =&gt; 150
Hashing 133 =&gt; 151
Hashing 134 =&gt; 152
Hashing 135 =&gt; 153
Hashing 136 =&gt; 154
Hashing 137 =&gt; 155
Hashing 138 =&gt; 156
Hashing 139 =&gt; 157
Hashing 140 =&gt; 149
Hashing 141 =&gt; 150
Hashing 142 =&gt; 151
Hashing 143 =&gt; 152
Hashing 144 =&gt; 153
Hashing 145 =&gt; 154
Hashing 146 =&gt; 155
Hashing 147 =&gt; 156
Hashing 148 =&gt; 157
Hashing 149 =&gt; 158
Hashing 150 =&gt; 150
Hashing 151 =&gt; 151
Hashing 152 =&gt; 152
Hashing 153 =&gt; 153
Hashing 154 =&gt; 154
Hashing 155 =&gt; 155
Hashing 156 =&gt; 156
Hashing 157 =&gt; 157
Hashing 158 =&gt; 158
Hashing 159 =&gt; 159
Hashing 160 =&gt; 151
Hashing 161 =&gt; 152
Hashing 162 =&gt; 153
Hashing 163 =&gt; 154
Hashing 164 =&gt; 155
Hashing 165 =&gt; 156
Hashing 166 =&gt; 157
Hashing 167 =&gt; 158
Hashing 168 =&gt; 159
Hashing 169 =&gt; 160
Hashing 170 =&gt; 152
Hashing 171 =&gt; 153
Hashing 172 =&gt; 154
Hashing 173 =&gt; 155
Hashing 174 =&gt; 156
Hashing 175 =&gt; 157
Hashing 176 =&gt; 158
Hashing 177 =&gt; 159
Hashing 178 =&gt; 160
Hashing 179 =&gt; 161
Hashing 180 =&gt; 153
Hashing 181 =&gt; 154
Hashing 182 =&gt; 155
Hashing 183 =&gt; 156
Hashing 184 =&gt; 157
Hashing 185 =&gt; 158
Hashing 186 =&gt; 159
Hashing 187 =&gt; 160
Hashing 188 =&gt; 161
Hashing 189 =&gt; 162
Hashing 190 =&gt; 154
Hashing 191 =&gt; 155
Hashing 192 =&gt; 156
Hashing 193 =&gt; 157
Hashing 194 =&gt; 158
Hashing 195 =&gt; 159
Hashing 196 =&gt; 160
Hashing 197 =&gt; 161
Hashing 198 =&gt; 162
Hashing 199 =&gt; 163
Hashing 200 =&gt; 146
Hashing 201 =&gt; 147
Hashing 202 =&gt; 148
Hashing 203 =&gt; 149
Hashing 204 =&gt; 150
Hashing 205 =&gt; 151
Hashing 206 =&gt; 152
Hashing 207 =&gt; 153
Hashing 208 =&gt; 154
Hashing 209 =&gt; 155
Hashing 210 =&gt; 147
Hashing 211 =&gt; 148
Hashing 212 =&gt; 149
Hashing 213 =&gt; 150
Hashing 214 =&gt; 151
Hashing 215 =&gt; 152
Hashing 216 =&gt; 153
Hashing 217 =&gt; 154
Hashing 218 =&gt; 155
Hashing 219 =&gt; 156
Hashing 220 =&gt; 148
Hashing 221 =&gt; 149
Hashing 222 =&gt; 150
Hashing 223 =&gt; 151
Hashing 224 =&gt; 152
Hashing 225 =&gt; 153
Hashing 226 =&gt; 154
Hashing 227 =&gt; 155
Hashing 228 =&gt; 156
Hashing 229 =&gt; 157
Hashing 230 =&gt; 149
Hashing 231 =&gt; 150
Hashing 232 =&gt; 151
Hashing 233 =&gt; 152
Hashing 234 =&gt; 153
Hashing 235 =&gt; 154
Hashing 236 =&gt; 155
Hashing 237 =&gt; 156
Hashing 238 =&gt; 157
Hashing 239 =&gt; 158
Hashing 240 =&gt; 150
Hashing 241 =&gt; 151
Hashing 242 =&gt; 152
Hashing 243 =&gt; 153
Hashing 244 =&gt; 154
Hashing 245 =&gt; 155
Hashing 246 =&gt; 156
Hashing 247 =&gt; 157
Hashing 248 =&gt; 158
Hashing 249 =&gt; 159
Hashing 250 =&gt; 151
Hashing 251 =&gt; 152
Hashing 252 =&gt; 153
Hashing 253 =&gt; 154
Hashing 254 =&gt; 155
Hashing 255 =&gt; 156
</code></pre>
<p>Notice the tendency towards higher numbers. That turns out to be our deadfall. Running the hash 4 times ($hash = ourHash($hash)`, for each element) winds up giving us:</p>
<pre><code>Hashing 0 =&gt; 153
Hashing 1 =&gt; 154
Hashing 2 =&gt; 155
Hashing 3 =&gt; 156
Hashing 4 =&gt; 157
Hashing 5 =&gt; 158
Hashing 6 =&gt; 150
Hashing 7 =&gt; 151
Hashing 8 =&gt; 152
Hashing 9 =&gt; 153
Hashing 10 =&gt; 157
Hashing 11 =&gt; 158
Hashing 12 =&gt; 150
Hashing 13 =&gt; 154
Hashing 14 =&gt; 155
Hashing 15 =&gt; 156
Hashing 16 =&gt; 157
Hashing 17 =&gt; 158
Hashing 18 =&gt; 150
Hashing 19 =&gt; 151
Hashing 20 =&gt; 158
Hashing 21 =&gt; 150
Hashing 22 =&gt; 154
Hashing 23 =&gt; 155
Hashing 24 =&gt; 156
Hashing 25 =&gt; 157
Hashing 26 =&gt; 158
Hashing 27 =&gt; 150
Hashing 28 =&gt; 151
Hashing 29 =&gt; 152
Hashing 30 =&gt; 150
Hashing 31 =&gt; 154
Hashing 32 =&gt; 155
Hashing 33 =&gt; 156
Hashing 34 =&gt; 157
Hashing 35 =&gt; 158
Hashing 36 =&gt; 150
Hashing 37 =&gt; 151
Hashing 38 =&gt; 152
Hashing 39 =&gt; 153
Hashing 40 =&gt; 154
Hashing 41 =&gt; 155
Hashing 42 =&gt; 156
Hashing 43 =&gt; 157
Hashing 44 =&gt; 158
Hashing 45 =&gt; 150
Hashing 46 =&gt; 151
Hashing 47 =&gt; 152
Hashing 48 =&gt; 153
Hashing 49 =&gt; 154
Hashing 50 =&gt; 155
Hashing 51 =&gt; 156
Hashing 52 =&gt; 157
Hashing 53 =&gt; 158
Hashing 54 =&gt; 150
Hashing 55 =&gt; 151
Hashing 56 =&gt; 152
Hashing 57 =&gt; 153
Hashing 58 =&gt; 154
Hashing 59 =&gt; 155
Hashing 60 =&gt; 156
Hashing 61 =&gt; 157
Hashing 62 =&gt; 158
Hashing 63 =&gt; 150
Hashing 64 =&gt; 151
Hashing 65 =&gt; 152
Hashing 66 =&gt; 153
Hashing 67 =&gt; 154
Hashing 68 =&gt; 155
Hashing 69 =&gt; 156
Hashing 70 =&gt; 157
Hashing 71 =&gt; 158
Hashing 72 =&gt; 150
Hashing 73 =&gt; 151
Hashing 74 =&gt; 152
Hashing 75 =&gt; 153
Hashing 76 =&gt; 154
Hashing 77 =&gt; 155
Hashing 78 =&gt; 156
Hashing 79 =&gt; 157
Hashing 80 =&gt; 158
Hashing 81 =&gt; 150
Hashing 82 =&gt; 151
Hashing 83 =&gt; 152
Hashing 84 =&gt; 153
Hashing 85 =&gt; 154
Hashing 86 =&gt; 155
Hashing 87 =&gt; 156
Hashing 88 =&gt; 157
Hashing 89 =&gt; 158
Hashing 90 =&gt; 150
Hashing 91 =&gt; 151
Hashing 92 =&gt; 152
Hashing 93 =&gt; 153
Hashing 94 =&gt; 154
Hashing 95 =&gt; 155
Hashing 96 =&gt; 156
Hashing 97 =&gt; 157
Hashing 98 =&gt; 158
Hashing 99 =&gt; 150
Hashing 100 =&gt; 154
Hashing 101 =&gt; 155
Hashing 102 =&gt; 156
Hashing 103 =&gt; 157
Hashing 104 =&gt; 158
Hashing 105 =&gt; 150
Hashing 106 =&gt; 151
Hashing 107 =&gt; 152
Hashing 108 =&gt; 153
Hashing 109 =&gt; 154
Hashing 110 =&gt; 155
Hashing 111 =&gt; 156
Hashing 112 =&gt; 157
Hashing 113 =&gt; 158
Hashing 114 =&gt; 150
Hashing 115 =&gt; 151
Hashing 116 =&gt; 152
Hashing 117 =&gt; 153
Hashing 118 =&gt; 154
Hashing 119 =&gt; 155
Hashing 120 =&gt; 156
Hashing 121 =&gt; 157
Hashing 122 =&gt; 158
Hashing 123 =&gt; 150
Hashing 124 =&gt; 151
Hashing 125 =&gt; 152
Hashing 126 =&gt; 153
Hashing 127 =&gt; 154
Hashing 128 =&gt; 155
Hashing 129 =&gt; 156
Hashing 130 =&gt; 157
Hashing 131 =&gt; 158
Hashing 132 =&gt; 150
Hashing 133 =&gt; 151
Hashing 134 =&gt; 152
Hashing 135 =&gt; 153
Hashing 136 =&gt; 154
Hashing 137 =&gt; 155
Hashing 138 =&gt; 156
Hashing 139 =&gt; 157
Hashing 140 =&gt; 158
Hashing 141 =&gt; 150
Hashing 142 =&gt; 151
Hashing 143 =&gt; 152
Hashing 144 =&gt; 153
Hashing 145 =&gt; 154
Hashing 146 =&gt; 155
Hashing 147 =&gt; 156
Hashing 148 =&gt; 157
Hashing 149 =&gt; 158
Hashing 150 =&gt; 150
Hashing 151 =&gt; 151
Hashing 152 =&gt; 152
Hashing 153 =&gt; 153
Hashing 154 =&gt; 154
Hashing 155 =&gt; 155
Hashing 156 =&gt; 156
Hashing 157 =&gt; 157
Hashing 158 =&gt; 158
Hashing 159 =&gt; 159
Hashing 160 =&gt; 151
Hashing 161 =&gt; 152
Hashing 162 =&gt; 153
Hashing 163 =&gt; 154
Hashing 164 =&gt; 155
Hashing 165 =&gt; 156
Hashing 166 =&gt; 157
Hashing 167 =&gt; 158
Hashing 168 =&gt; 159
Hashing 169 =&gt; 151
Hashing 170 =&gt; 152
Hashing 171 =&gt; 153
Hashing 172 =&gt; 154
Hashing 173 =&gt; 155
Hashing 174 =&gt; 156
Hashing 175 =&gt; 157
Hashing 176 =&gt; 158
Hashing 177 =&gt; 159
Hashing 178 =&gt; 151
Hashing 179 =&gt; 152
Hashing 180 =&gt; 153
Hashing 181 =&gt; 154
Hashing 182 =&gt; 155
Hashing 183 =&gt; 156
Hashing 184 =&gt; 157
Hashing 185 =&gt; 158
Hashing 186 =&gt; 159
Hashing 187 =&gt; 151
Hashing 188 =&gt; 152
Hashing 189 =&gt; 153
Hashing 190 =&gt; 154
Hashing 191 =&gt; 155
Hashing 192 =&gt; 156
Hashing 193 =&gt; 157
Hashing 194 =&gt; 158
Hashing 195 =&gt; 159
Hashing 196 =&gt; 151
Hashing 197 =&gt; 152
Hashing 198 =&gt; 153
Hashing 199 =&gt; 154
Hashing 200 =&gt; 155
Hashing 201 =&gt; 156
Hashing 202 =&gt; 157
Hashing 203 =&gt; 158
Hashing 204 =&gt; 150
Hashing 205 =&gt; 151
Hashing 206 =&gt; 152
Hashing 207 =&gt; 153
Hashing 208 =&gt; 154
Hashing 209 =&gt; 155
Hashing 210 =&gt; 156
Hashing 211 =&gt; 157
Hashing 212 =&gt; 158
Hashing 213 =&gt; 150
Hashing 214 =&gt; 151
Hashing 215 =&gt; 152
Hashing 216 =&gt; 153
Hashing 217 =&gt; 154
Hashing 218 =&gt; 155
Hashing 219 =&gt; 156
Hashing 220 =&gt; 157
Hashing 221 =&gt; 158
Hashing 222 =&gt; 150
Hashing 223 =&gt; 151
Hashing 224 =&gt; 152
Hashing 225 =&gt; 153
Hashing 226 =&gt; 154
Hashing 227 =&gt; 155
Hashing 228 =&gt; 156
Hashing 229 =&gt; 157
Hashing 230 =&gt; 158
Hashing 231 =&gt; 150
Hashing 232 =&gt; 151
Hashing 233 =&gt; 152
Hashing 234 =&gt; 153
Hashing 235 =&gt; 154
Hashing 236 =&gt; 155
Hashing 237 =&gt; 156
Hashing 238 =&gt; 157
Hashing 239 =&gt; 158
Hashing 240 =&gt; 150
Hashing 241 =&gt; 151
Hashing 242 =&gt; 152
Hashing 243 =&gt; 153
Hashing 244 =&gt; 154
Hashing 245 =&gt; 155
Hashing 246 =&gt; 156
Hashing 247 =&gt; 157
Hashing 248 =&gt; 158
Hashing 249 =&gt; 159
Hashing 250 =&gt; 151
Hashing 251 =&gt; 152
Hashing 252 =&gt; 153
Hashing 253 =&gt; 154
Hashing 254 =&gt; 155
Hashing 255 =&gt; 156
</code></pre>
<p>We've narrowed ourselves down to 8 values... That's <strong>bad</strong>... Our original function mapped <code>S(∞)</code> onto <code>S(256)</code>. That is we've created a <a href="http://en.wikipedia.org/wiki/Surjection" rel="noreferrer">Surjective Function</a> mapping <code>$input</code> to <code>$output</code>. </p>
<p>Since we have a Surjective function, we have no guarantee the mapping for any subset of the input won't have collisions (in fact, in practice they will).</p>
<p>That's what happened here! Our function was bad, but that's not why this worked (that's why it worked so quickly and so completely). </p>
<p>The same thing happens with <code>MD5</code>. It maps <code>S(∞)</code> onto <code>S(2^128)</code>. Since there's no guarantee that running <code>MD5(S(output))</code> will be <a href="http://en.wikipedia.org/wiki/Injective_function" rel="noreferrer">Injective</a>, meaning that it won't have collisions.</p>
<h2>TL/DR Section</h2>
<p>Therefore, since feeding the output back to <code>md5</code> directly can generate collisions, every iteration will increase the chance of collisions. This is a linear increase however, which means that while the result set of <code>2^128</code> is reduced, it's not significantly reduced fast enough to be a critical flaw.</p>
<p>So,</p>
<pre><code>$output = md5($input); // 2^128 possibilities
$output = md5($output); // &lt; 2^128 possibilities
$output = md5($output); // &lt; 2^128 possibilities
$output = md5($output); // &lt; 2^128 possibilities
$output = md5($output); // &lt; 2^128 possibilities
</code></pre>
<p>The more times you iterate, the further the reduction goes.</p>
<h1>The Fix</h1>
<p>Fortunately for us, there's a <em>trivial</em> way to fix this: Feed back <em>something</em> into the further iterations:</p>
<pre><code>$output = md5($input); // 2^128 possibilities
$output = md5($input . $output); // 2^128 possibilities
$output = md5($input . $output); // 2^128 possibilities
$output = md5($input . $output); // 2^128 possibilities
$output = md5($input . $output); // 2^128 possibilities    
</code></pre>
<p>Note that the further iterations aren't 2^128 for each individual value for <code>$input</code>. Meaning that we may be able to generate <code>$input</code> values that still collide down the line (and hence will settle or resonate at far less than <code>2^128</code> possible outputs). But the general case for <code>$input</code> is still as strong as it was for a single round.</p>
<p>Wait, was it? Let's test this out with our <code>ourHash()</code> function. Switching to <code>$hash = ourHash($input . $hash);</code>, for 100 iterations:</p>
<pre><code>Hashing 0 =&gt; 201
Hashing 1 =&gt; 212
Hashing 2 =&gt; 199
Hashing 3 =&gt; 201
Hashing 4 =&gt; 203
Hashing 5 =&gt; 205
Hashing 6 =&gt; 207
Hashing 7 =&gt; 209
Hashing 8 =&gt; 211
Hashing 9 =&gt; 204
Hashing 10 =&gt; 251
Hashing 11 =&gt; 147
Hashing 12 =&gt; 251
Hashing 13 =&gt; 148
Hashing 14 =&gt; 253
Hashing 15 =&gt; 0
Hashing 16 =&gt; 1
Hashing 17 =&gt; 2
Hashing 18 =&gt; 161
Hashing 19 =&gt; 163
Hashing 20 =&gt; 147
Hashing 21 =&gt; 251
Hashing 22 =&gt; 148
Hashing 23 =&gt; 253
Hashing 24 =&gt; 0
Hashing 25 =&gt; 1
Hashing 26 =&gt; 2
Hashing 27 =&gt; 161
Hashing 28 =&gt; 163
Hashing 29 =&gt; 8
Hashing 30 =&gt; 251
Hashing 31 =&gt; 148
Hashing 32 =&gt; 253
Hashing 33 =&gt; 0
Hashing 34 =&gt; 1
Hashing 35 =&gt; 2
Hashing 36 =&gt; 161
Hashing 37 =&gt; 163
Hashing 38 =&gt; 8
Hashing 39 =&gt; 4
Hashing 40 =&gt; 148
Hashing 41 =&gt; 253
Hashing 42 =&gt; 0
Hashing 43 =&gt; 1
Hashing 44 =&gt; 2
Hashing 45 =&gt; 161
Hashing 46 =&gt; 163
Hashing 47 =&gt; 8
Hashing 48 =&gt; 4
Hashing 49 =&gt; 9
Hashing 50 =&gt; 253
Hashing 51 =&gt; 0
Hashing 52 =&gt; 1
Hashing 53 =&gt; 2
Hashing 54 =&gt; 161
Hashing 55 =&gt; 163
Hashing 56 =&gt; 8
Hashing 57 =&gt; 4
Hashing 58 =&gt; 9
Hashing 59 =&gt; 11
Hashing 60 =&gt; 0
Hashing 61 =&gt; 1
Hashing 62 =&gt; 2
Hashing 63 =&gt; 161
Hashing 64 =&gt; 163
Hashing 65 =&gt; 8
Hashing 66 =&gt; 4
Hashing 67 =&gt; 9
Hashing 68 =&gt; 11
Hashing 69 =&gt; 4
Hashing 70 =&gt; 1
Hashing 71 =&gt; 2
Hashing 72 =&gt; 161
Hashing 73 =&gt; 163
Hashing 74 =&gt; 8
Hashing 75 =&gt; 4
Hashing 76 =&gt; 9
Hashing 77 =&gt; 11
Hashing 78 =&gt; 4
Hashing 79 =&gt; 3
Hashing 80 =&gt; 2
Hashing 81 =&gt; 161
Hashing 82 =&gt; 163
Hashing 83 =&gt; 8
Hashing 84 =&gt; 4
Hashing 85 =&gt; 9
Hashing 86 =&gt; 11
Hashing 87 =&gt; 4
Hashing 88 =&gt; 3
Hashing 89 =&gt; 17
Hashing 90 =&gt; 161
Hashing 91 =&gt; 163
Hashing 92 =&gt; 8
Hashing 93 =&gt; 4
Hashing 94 =&gt; 9
Hashing 95 =&gt; 11
Hashing 96 =&gt; 4
Hashing 97 =&gt; 3
Hashing 98 =&gt; 17
Hashing 99 =&gt; 13
Hashing 100 =&gt; 246
Hashing 101 =&gt; 248
Hashing 102 =&gt; 49
Hashing 103 =&gt; 44
Hashing 104 =&gt; 255
Hashing 105 =&gt; 198
Hashing 106 =&gt; 43
Hashing 107 =&gt; 51
Hashing 108 =&gt; 202
Hashing 109 =&gt; 2
Hashing 110 =&gt; 248
Hashing 111 =&gt; 49
Hashing 112 =&gt; 44
Hashing 113 =&gt; 255
Hashing 114 =&gt; 198
Hashing 115 =&gt; 43
Hashing 116 =&gt; 51
Hashing 117 =&gt; 202
Hashing 118 =&gt; 2
Hashing 119 =&gt; 51
Hashing 120 =&gt; 49
Hashing 121 =&gt; 44
Hashing 122 =&gt; 255
Hashing 123 =&gt; 198
Hashing 124 =&gt; 43
Hashing 125 =&gt; 51
Hashing 126 =&gt; 202
Hashing 127 =&gt; 2
Hashing 128 =&gt; 51
Hashing 129 =&gt; 53
Hashing 130 =&gt; 44
Hashing 131 =&gt; 255
Hashing 132 =&gt; 198
Hashing 133 =&gt; 43
Hashing 134 =&gt; 51
Hashing 135 =&gt; 202
Hashing 136 =&gt; 2
Hashing 137 =&gt; 51
Hashing 138 =&gt; 53
Hashing 139 =&gt; 55
Hashing 140 =&gt; 255
Hashing 141 =&gt; 198
Hashing 142 =&gt; 43
Hashing 143 =&gt; 51
Hashing 144 =&gt; 202
Hashing 145 =&gt; 2
Hashing 146 =&gt; 51
Hashing 147 =&gt; 53
Hashing 148 =&gt; 55
Hashing 149 =&gt; 58
Hashing 150 =&gt; 198
Hashing 151 =&gt; 43
Hashing 152 =&gt; 51
Hashing 153 =&gt; 202
Hashing 154 =&gt; 2
Hashing 155 =&gt; 51
Hashing 156 =&gt; 53
Hashing 157 =&gt; 55
Hashing 158 =&gt; 58
Hashing 159 =&gt; 0
Hashing 160 =&gt; 43
Hashing 161 =&gt; 51
Hashing 162 =&gt; 202
Hashing 163 =&gt; 2
Hashing 164 =&gt; 51
Hashing 165 =&gt; 53
Hashing 166 =&gt; 55
Hashing 167 =&gt; 58
Hashing 168 =&gt; 0
Hashing 169 =&gt; 209
Hashing 170 =&gt; 51
Hashing 171 =&gt; 202
Hashing 172 =&gt; 2
Hashing 173 =&gt; 51
Hashing 174 =&gt; 53
Hashing 175 =&gt; 55
Hashing 176 =&gt; 58
Hashing 177 =&gt; 0
Hashing 178 =&gt; 209
Hashing 179 =&gt; 216
Hashing 180 =&gt; 202
Hashing 181 =&gt; 2
Hashing 182 =&gt; 51
Hashing 183 =&gt; 53
Hashing 184 =&gt; 55
Hashing 185 =&gt; 58
Hashing 186 =&gt; 0
Hashing 187 =&gt; 209
Hashing 188 =&gt; 216
Hashing 189 =&gt; 219
Hashing 190 =&gt; 2
Hashing 191 =&gt; 51
Hashing 192 =&gt; 53
Hashing 193 =&gt; 55
Hashing 194 =&gt; 58
Hashing 195 =&gt; 0
Hashing 196 =&gt; 209
Hashing 197 =&gt; 216
Hashing 198 =&gt; 219
Hashing 199 =&gt; 220
Hashing 200 =&gt; 248
Hashing 201 =&gt; 49
Hashing 202 =&gt; 44
Hashing 203 =&gt; 255
Hashing 204 =&gt; 198
Hashing 205 =&gt; 43
Hashing 206 =&gt; 51
Hashing 207 =&gt; 202
Hashing 208 =&gt; 2
Hashing 209 =&gt; 51
Hashing 210 =&gt; 49
Hashing 211 =&gt; 44
Hashing 212 =&gt; 255
Hashing 213 =&gt; 198
Hashing 214 =&gt; 43
Hashing 215 =&gt; 51
Hashing 216 =&gt; 202
Hashing 217 =&gt; 2
Hashing 218 =&gt; 51
Hashing 219 =&gt; 53
Hashing 220 =&gt; 44
Hashing 221 =&gt; 255
Hashing 222 =&gt; 198
Hashing 223 =&gt; 43
Hashing 224 =&gt; 51
Hashing 225 =&gt; 202
Hashing 226 =&gt; 2
Hashing 227 =&gt; 51
Hashing 228 =&gt; 53
Hashing 229 =&gt; 55
Hashing 230 =&gt; 255
Hashing 231 =&gt; 198
Hashing 232 =&gt; 43
Hashing 233 =&gt; 51
Hashing 234 =&gt; 202
Hashing 235 =&gt; 2
Hashing 236 =&gt; 51
Hashing 237 =&gt; 53
Hashing 238 =&gt; 55
Hashing 239 =&gt; 58
Hashing 240 =&gt; 198
Hashing 241 =&gt; 43
Hashing 242 =&gt; 51
Hashing 243 =&gt; 202
Hashing 244 =&gt; 2
Hashing 245 =&gt; 51
Hashing 246 =&gt; 53
Hashing 247 =&gt; 55
Hashing 248 =&gt; 58
Hashing 249 =&gt; 0
Hashing 250 =&gt; 43
Hashing 251 =&gt; 51
Hashing 252 =&gt; 202
Hashing 253 =&gt; 2
Hashing 254 =&gt; 51
Hashing 255 =&gt; 53
</code></pre>
<p>There's still a rough pattern there, but note that it's no <em>more</em> of a pattern than our underlying function (which was already quite weak). </p>
<p>Notice however that <code>0</code> and <code>3</code> became collisions, even though they weren't in the single run. That's an application of what I said before (that the collision resistance stays the same for the set of all inputs, but specific collision routes may open up due to flaws in the underlying algorithm).</p>
<h2>TL/DR Section</h2>
<p>By feeding back the input into each iteration, we effectively break any collisions that may have occurred in the prior iteration.</p>
<p>Therefore, <code>md5($input . md5($input));</code> should be (<em>theoretically</em> at least) as strong as <code>md5($input)</code>. </p>
<h1>Is This Important?</h1>
<p>Yes. This is one of the reasons that PBKDF2 replaced PBKDF1 in <a href="http://tools.ietf.org/html/rfc2898#section-5.1" rel="noreferrer">RFC 2898</a>. Consider the inner loops of the two::</p>
<p>PBKDF1:</p>
<pre><code>T_1 = Hash (P || S) ,
T_2 = Hash (T_1) ,
...
T_c = Hash (T_{c-1}) 
</code></pre>
<p>Where <code>c</code> is the iteration count, <code>P</code> is the Password and <code>S</code> is the salt</p>
<p>PBKDF2:</p>
<pre><code>U_1 = PRF (P, S || INT (i)) ,
U_2 = PRF (P, U_1) ,
...
U_c = PRF (P, U_{c-1})
</code></pre>
<p>Where PRF is really just a HMAC. But for our purposes here, let's just say that <code>PRF(P, S) = Hash(P || S)</code> (that is, the PRF of 2 inputs is the same, roughly speaking, as hash with the two concatenated together). It's very much <strong>not</strong>, but for our purposes it is.</p>
<p>So PBKDF2 maintains the collision resistance of the underlying <code>Hash</code> function, where PBKDF1 does not.</p>
<h1>Tie-ing All Of It Together:</h1>
<p>We know of secure ways of iterating a hash. In fact:</p>
<pre><code>$hash = $input;
$i = 10000;
do {
   $hash = hash($input . $hash);
} while ($i-- &gt; 0);
</code></pre>
<p>Is typically safe. </p>
<p>Now, to go into <strong>why</strong> we would want to hash it, let's analyze the entropy movement.</p>
<p>A hash takes in the infinite set: <code>S(∞)</code> and produces a smaller, consistently sized set <code>S(n)</code>. The next iteration (assuming the input is passed back in) maps <code>S(∞)</code> onto <code>S(n)</code> again:</p>
<pre><code>S(∞) -&gt; S(n)
S(∞) -&gt; S(n)
S(∞) -&gt; S(n)
S(∞) -&gt; S(n)
S(∞) -&gt; S(n)
S(∞) -&gt; S(n)
</code></pre>
<p>Notice that the final output has <strong>exactly the same amount of entropy as the first one</strong>. Iterating will <strong>not</strong> "make it more obscured". The entropy is identical. There's no magic source of unpredictability (it's a Pseudo-Random-Function, not a Random Function).</p>
<p>There is however a gain to iterating. It makes the hashing process artificially slower. And that's why iterating can be a good idea. In fact, it's the basic principle of most modern password hashing algorithms (the fact that doing something over-and-over makes it slower).</p>
<p>Slow is good, because it's combating the primary security threat: brute forcing. The slower we make our hashing algorithm, the harder attackers have to work to attack password hashes stolen from us. And that's a good thing!!!</p>
</div></text><author><a href="/users/338665/ircmaxell">ircmaxell</a></author><comments><comment><text><span class="comment-copy"><code>$output = md5($output); // &lt; 2^128 possibilities</code> --- is it really strict <code>&lt;</code>, or <code>&lt;=</code>?</span></text><author><a class="comment-user" href="/users/251311/zerkms" title="158,727 reputation">zerkms</a></author></comment><comment><text><span class="comment-copy">@zerkms: It's not strictly anything. We'd need to know some very specific details of the underlying function (<code>md5()</code> in this case) to really know for sure. But in general it will be <code>&lt;</code> and not <code>&lt;=</code>... Remember, we're talking about the size of the set of <code>$output</code> for <i>all</i> possible <code>$inputs</code>. So if we have even <i>one</i> collision it will be <code>&lt;</code>, therefore <code>&lt;</code> is the better generalizer.</span></text><author><a class="comment-user" href="/users/338665/ircmaxell" title="119,301 reputation">ircmaxell</a></author></comment><comment><text><span class="comment-copy">"So if we have even one collision it will be &lt;" --- there is no math proof for that :-)</span></text><author><a class="comment-user" href="/users/251311/zerkms" title="158,727 reputation">zerkms</a></author></comment><comment><text><span class="comment-copy">@zerkms: yes there is. If there is even one collision it ceases to be a Injective Function. And you require a purely Injective function for <code>=</code> (as every input in <code>S(2^128)</code> would need to map to exactly one output in <code>S(2^128)</code>). But having <i>one</i> collision means it's no longer Injective, and hence the output space by mandate becomes smaller than the input space (by the very definition of the function itself)...</span></text><author><a class="comment-user" href="/users/338665/ircmaxell" title="119,301 reputation">ircmaxell</a></author></comment><comment><text><span class="comment-copy">"If there is even one collision" --- so how would you know there is one?</span></text><author><a class="comment-user" href="/users/251311/zerkms" title="158,727 reputation">zerkms</a></author></comment><comment><text><span class="comment-copy">Thanks for this insight! However, I am wondering if this specific collision also applies for hash functions where the output size is the same size as the internal state, for example: is running SHA512 twice not the same as adding more rounds to the hash functions, and therefore should be ok because the surjective mapping does not occur?</span></text><author><a class="comment-user" href="/users/658536/tim" title="637 reputation">Tim</a></author></comment><comment><text><span class="comment-copy">@Tim: the internal state doesn't matter to this analysis. The only states that matter are input (all less than 2^128-1) and the output (2^10). So yes, this still applies to SHA-512...</span></text><author><a class="comment-user" href="/users/338665/ircmaxell" title="119,301 reputation">ircmaxell</a></author></comment><comment><text><span class="comment-copy">Really liked this answer</span></text><author><a class="comment-user" href="/users/863063/abidibo" title="995 reputation">abidibo</a></author></comment><comment><text><span class="comment-copy">zerkms: It's called Birthday problem. There is 2^128 different md5 hashes (that's fact - based on combinatorics). So if you pick 2^128 different inputs, all of them generating unique hash you would use all of the possible hashes. So if you picked 2^128+1 you would be left with the one and all hashes used =&gt; the one will have same hash as one of the 2^128. Therefore there is at least one collision. By mathematical induction you can prove, that there is actually infinite number of collisions.</span></text><author><a class="comment-user" href="/users/112000/tom%c3%a1%c5%a1-fejfar" title="7,854 reputation">Tomáš Fejfar</a></author></comment><comment><text><span class="comment-copy">@TomášFejfar I think the question is not about collisions in general, but collisions in the strict output set (2^128 outputs, each exactly 128 bits wide). That <i>could</i> be Injective, but as far as I know a generic proof is not possible (only a proof-by-example of a collision for a specific algorithm). Consider the hash function that simply returns the input if it is 128 bits (and hashes otherwise). In general it would be surjective, but when fed its output it would always be injective... That's the point of contention...</span></text><author><a class="comment-user" href="/users/338665/ircmaxell" title="119,301 reputation">ircmaxell</a></author></comment><comment><text><span class="comment-copy">Can you elaborate what approximately would be the output size of 1000 iterations of <code>$input = MD5($input)</code>? For one iteration, it's 16^32 ~ 3e38. If MD5 behaves like a random function then for 1000 iterations, it should be around 7e35.</span></text><author><a class="comment-user" href="/users/783580/jakub-vr%c3%a1na" title="376 reputation">Jakub Vrána</a></author></comment><comment><text><span class="comment-copy">This is a really good answer. Thanks for clearing this up for me. +1</span></text><author><a class="comment-user" href="/users/921563/optimuscrime" title="8,820 reputation">OptimusCrime</a></author></comment><comment><text><span class="comment-copy">@ircmaxell, +1 for the effort and elaborateness, but  parts of  <b>this answer is very very wrong</b>  / misleading. There is exactly zero proof that <code>md5($input . md5($input))</code> maps to <code>S(n)</code> and it's not even theoretically correct. The theoretically correct equation is <code>md5(∞ . md5($input)) = S(n)</code> and since <code>$input</code> is <b>far from infinity</b> there is simply no way to prove that  <code>md5($input . md5($input))</code> has more entropy than <code>md5(md5($input))</code>. Both equations are equally bad and impossible to be proved to map to <code>S(n)</code>. I would hope you revise the answer since it's so highly voted.</span></text><author><a class="comment-user" href="/users/632951/pacerier" title="33,795 reputation">Pacerier</a></author></comment><comment><text><span class="comment-copy"><code>$hash = hash($input . $hash);</code> Wouldn't brute forcing a single iteration of the stored hash give you e.g. "MyPassword5f4dcc3b5aa765d61d8327deb882cf99"? Why bother going further when I can just read the original input from the prefix of that string? You need to use something other than the original input or this whole exercise is futile.</span></text><author><a class="comment-user" href="/users/770230/dan" title="2,705 reputation">Dan</a></author></comment><comment><text><span class="comment-copy">Let us <a href="http://chat.stackoverflow.com/rooms/66609/discussion-between-ircmaxell-and-dan">continue this discussion in chat</a>.</span></text><author><a class="comment-user" href="/users/338665/ircmaxell" title="119,301 reputation">ircmaxell</a></author></comment><comment><text><span class="comment-copy">For those who would like to save time by not having to go check how that discussion between Dan &amp; ircmaxell finished, <b>it finished well</b>: Dan agreeing with ircmaxell.</span></text><author><a class="comment-user" href="/users/1524913/jeromej" title="2,045 reputation">JeromeJ</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>Yes, re-hashing reduces the search space, but no, it doesn't matter - the effective reduction is insignificant.</p>
<p>Re-hashing increases the time it takes to brute-force, but doing so only twice is also suboptimal.</p>
<p>What you really want is to hash the password with <a href="http://tools.ietf.org/html/rfc2898#section-5.2" rel="noreferrer">PBKDF2</a> - a proven method of using a secure hash with salt and iterations. Check out <a href="http://stackoverflow.com/questions/287517/encryptinghashing-plain-text-passwords-in-database#287883">this SO response</a>.</p>
<p><b>EDIT</b>: I almost forgot - <b>DON'T USE MD5!!!!</b> Use a modern cryptographic hash such as the SHA-2 family (SHA-256, SHA-384, and SHA-512).</p>
</div></text><author><a href="/users/37020/orip">orip</a></author><comments><comment><text><span class="comment-copy">Or BCrypt, that always works.</span></text><author><a class="comment-user" href="/users/881421/dftr" title="509 reputation">DFTR</a></author></comment><comment><text><span class="comment-copy">@DFTR - agreed. bcrypt or scrypt are better options.</span></text><author><a class="comment-user" href="/users/37020/orip" title="40,336 reputation">orip</a></author></comment><comment><text><span class="comment-copy">+1 for mentioning DON'T USE MD5</span></text><author><a class="comment-user" href="/users/16964/cyberherbalist" title="6,468 reputation">Cyberherbalist</a></author></comment><comment><text><span class="comment-copy">Don't use those either (SHA-2 family) they can now also be cracked easily, check <a href="https://crackstation.net/" rel="nofollow noreferrer">crackstation.net</a> for proof. If anything use scrypt or PBKDF2 which are key derivation function (KDFs) based cryptographic hash functions.</span></text><author><a class="comment-user" href="/users/1148795/thoggy" title="248 reputation">thoggy</a></author></comment><comment><text><span class="comment-copy">In 2016, Argon2 and scrypt are the ones everyone should strive to use</span></text><author><a class="comment-user" href="/users/633098/silkfire" title="10,697 reputation">silkfire</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>Yes - it reduces the number of possibly strings that match the string.</p>
<p>As you have already mentioned, salted hashes are much better.</p>
<p>An article here: <a href="http://websecurity.ro/blog/2007/11/02/md5md5-vs-md5/" rel="noreferrer">http://websecurity.ro/blog/2007/11/02/md5md5-vs-md5/</a>, attempts a proof at why it is equivalent, but I'm not sure with the logic. Partly they assume that there isn't software available to analyse md5(md5(text)), but obviously it's fairly trivial to produce the rainbow tables.</p>
<p>I'm still sticking with my answer that there are smaller number of md5(md5(text)) type hashes than md5(text) hashes, increasing the chance of collision (even if still to an unlikely probability) and reducing the search space.</p>
</div></text><author><a href="/users/16511/rich-bradshaw">Rich Bradshaw</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>I just look at this from a practical standpoint. What is the hacker after? Why, the combination of characters that, when put through the hash function, generates the desired hash.</p>
<p>You are only saving the last hash, therefore, the hacker only has to bruteforce one hash. Assuming you have roughly the same odds of stumbling across the desired hash with each bruteforce step, the number of hashes is irrelevant. You could do a million hash iterations, and it would not increase or reduce security one bit, since at the end of the line there's still only one hash to break, and the odds of breaking it are the same as any hash.</p>
<p>Maybe the previous posters think that the input is relevant; it's not. As long as whatever you put into the hash function generates the desired hash, it will get you through, correct input or incorrect input.</p>
<p>Now, rainbow tables are another story. Since a rainbow table only carries raw passwords, hashing twice may be a good security measure, since a rainbow table that contains every hash of every hash would be too large.</p>
<p>Of course, I'm only considering the example the OP gave, where it's just a plain-text password being hashed. If you include the username or a salt in the hash, it's a different story; hashing twice is entirely unnecessary, since the rainbow table would already be too large to be practical and contain the right hash.</p>
<p>Anyway, not a security expert here, but that's just what I've figured from my experience.</p>
</div></text><author><a href="/users/928419/pat">Pat</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>Personally I wouldn't bother with multiple hashses, but I'd make sure to <strong>also hash the UserName (or another User ID field) as well as the password</strong> so two users with the same password won't end up with the same hash. Also I'd probably throw some other constant string into the input string too for good measure.</p>
<pre><code>$hashed_password = md5( "xxx" + "|" + user_name + "|" + plaintext_password);
</code></pre>
</div></text><author><a href="/users/26335/codeandcats">CodeAndCats</a></author><comments><comment><text><span class="comment-copy">Actually, it should be a string randomly generated for each user, not a constant.</span></text><author><a class="comment-user owner" href="/users/1288/bill-the-lizard" title="240,849 reputation">Bill the Lizard</a></author></comment><comment><text><span class="comment-copy">A constant secret works (and is easier to work with), if you throw in the username as suggested. That essentially produces a random user-specific key.</span></text><author><a class="comment-user" href="/users/15962/squarecog" title="15,914 reputation">SquareCog</a></author></comment><comment><text><span class="comment-copy">A constant secret salt is security through obscurity.  If the "secret" gets out that you're using "xxx" + username + password, then an attacker doesn't even need data from your tables to launch an attack against it.</span></text><author><a class="comment-user owner" href="/users/1288/bill-the-lizard" title="240,849 reputation">Bill the Lizard</a></author></comment><comment><text><span class="comment-copy">I don't think that it's security through obscurity. The reason for using a salt is that you can't compute a rainbow table against multiple md5 hashes simultaneously. Building one for "xxx"+password (same salt) happens once. Building a table for "xxx"+username+password is worse than brute forcing.</span></text><author><a class="comment-user" href="/users/28776/fryguy" title="7,157 reputation">FryGuy</a></author></comment><comment><text><span class="comment-copy">@FryGuy: usernames aren't secret.  If the "secret" salt xxx is used, the attack is reduced to building one dictionary to attack a specific username.  If you use random salts then your database would have to be compromised before the same attack would be possible.</span></text><author><a class="comment-user owner" href="/users/1288/bill-the-lizard" title="240,849 reputation">Bill the Lizard</a></author></comment><comment><text><span class="comment-copy">@Bill the Lizard: "the attack is reduced to building one dictionary to attack a specific username" is just a brute-force attack (actually even worse, because in addition to computing all hashes you have to store them), so the salt works perfectly in this case.</span></text><author><a class="comment-user" href="/users/27009/kornel" title="66,242 reputation">Kornel</a></author></comment><comment><text><span class="comment-copy">Wow, I'm starting to feel a wee bit naive/ignorant on this topic! What the heck is a rainbow table? lol I'm finding these comments a bit hard to follow, not sure what we're assuming the hacker has access to and how they're attacking. I guess I need to do some more research on this! :)</span></text><author><a class="comment-user" href="/users/26335/codeandcats" title="3,290 reputation">CodeAndCats</a></author></comment><comment><text><span class="comment-copy">@Ben Daniel, a <a href="http://en.wikipedia.org/wiki/Rainbow_table" rel="nofollow noreferrer">rainbow table</a> is a precomputed list of known hashes and what plaintext resulted in those hashes.</span></text><author><a class="comment-user" href="/users/209744/pp" title="8,299 reputation">PP.</a></author></comment><comment><text><span class="comment-copy">@porneL: the problem with that argument is that it assumes that brute-forcing is going to be slow... So no, you don't have to store those hashes. And considering current attack rates for <code>md5()</code> are around 180 billion per second, even your secure 9 character password will fall in approximately 20 days (different cases, numbers and symbols, to a 50% chance of hitting it). The reason not to use "usernames", is that another site may be as well, and then an attacker could amortize the attacking against both hashes at the same time (assuming you're using a different pass on each site)... Random FTW</span></text><author><a class="comment-user" href="/users/338665/ircmaxell" title="119,301 reputation">ircmaxell</a></author></comment><comment><text><span class="comment-copy">@ircmaxell attacker could amortize search only if hashes from different sites have same common prefix, so if the <code>"xxx"</code> part is unique, it'll foil that. And yes, MD5 is fast, but question was about single vs double hashing and that alone doesn't change order of magnitude of computation needed, unlike e.g. <a href="http://en.wikipedia.org/wiki/Key_stretching" rel="nofollow noreferrer">proper key stretching</a>.</span></text><author><a class="comment-user" href="/users/27009/kornel" title="66,242 reputation">Kornel</a></author></comment><comment><text><span class="comment-copy">Don't use MD5 at all.</span></text><author><a class="comment-user" href="/users/999305/user999305" title="641 reputation">user999305</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>Most answers are by people without a background in cryptography or security. And they are wrong. Use a salt, if possible unique per record. MD5/SHA/etc are too fast, the opposite of what you want. PBKDF2 and bcrypt are slower (wich is good) but can be defeated with ASICs/FPGA/GPUs (very afordable nowadays). So a memory-hard algorithm is needed: <a href="http://www.tarsnap.com/scrypt/scrypt.pdf" rel="nofollow">enter scrypt</a>.</p>
<p>Here's a <a href="http://chargen.matasano.com/chargen/2007/9/7/enough-with-the-rainbow-tables-what-you-need-to-know-about-s.html" rel="nofollow">layman explanation</a> on salts and speed (but not about memory-hard algorithms).</p>
</div></text><author><a href="/users/169437/alecco">alecco</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>In general, it provides no additional security to double hash or double encrypt something.  If you can break the hash once, you can break it again.  It usually doesn't hurt security to do this, though.</p>
<p>In your example of using MD5, as you probably know there are some collision issues.  "Double Hashing" doesn't really help protect against this, since the same collisions will still result in the same first hash, which you can then MD5 again to get the second hash.</p>
<p>This does protect against dictionary attacks, like those "reverse MD5-databases", but so does salting.</p>
<p>On a tangent, Double encrypting something doesn't provide any additional security because all it does is result in a different key which is a combination of the two keys actually used.  So the effort to find the "key" is not doubled because two keys do not actually need to be found.  This isn't true for hashing, because the result of the hash is not usually the same length as the original input.</p>
</div></text><author><a href="/users/36384/soapbox">SoapBox</a></author><comments><comment><text><span class="comment-copy">All correct, but I just want to note that the effect of the strong collision resistance compromise on MD5 is blown a bit out of proportion -- most scenarios that use crypto hash functions do not rely on strong collision resistance, just weak resistance. They are not affected by this vulnerability.</span></text><author><a class="comment-user" href="/users/15962/squarecog" title="15,914 reputation">SquareCog</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>From what I've read, it may actually be recommended to re-hash the password hundreds or thousands of times.  </p>
<p>The idea is that if you can make it take more time to encode the password, it's more work for an attacker to run through many guesses to crack the password.  That seems to be the advantage to re-hashing -- not that it's more cryptographically secure, but it simply takes longer to generate a dictionary attack.</p>
<p>Of course computers get faster all the time, so this advantage diminishes over time (or requires you to increase the iterations).</p>
</div></text><author><a href="/users/20860/bill-karwin">Bill Karwin</a></author><comments><comment><text><span class="comment-copy">I mentioned this in another comment too, but  <a href="http://en.wikipedia.org/wiki/Key_stretching" rel="nofollow noreferrer">en.wikipedia.org/wiki/Key_stretching</a></span></text><author><a class="comment-user" href="/users/41871/willie-wheeler" title="16,772 reputation">Willie Wheeler</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>As several responses in this article suggest, there are some cases where it may improves security and others where it definately hurts it. There is a better solution that will definately improve security. Instead of doubling the number of times you calculate the hash, double the size of your salt, or double the number of bits used int the hash, or do both! Instead of SHA-245, jump up to SHA-512.</p>
</div></text><author><a href="/users/19704/stefan-rusek">Stefan Rusek</a></author><comments><comment><text><span class="comment-copy">This doesn't answer the question.</span></text><author><a class="comment-user owner" href="/users/1288/bill-the-lizard" title="240,849 reputation">Bill the Lizard</a></author></comment><comment><text><span class="comment-copy">Double hashing is not worth the effort, but doubling your hash size is. I think this is a more valuable point.</span></text><author><a class="comment-user" href="/users/19704/stefan-rusek" title="3,420 reputation">Stefan Rusek</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>Let us assume you use the hashing algorithm: compute rot13, take the first 10 characters. If you do that twice (or even 2000 times) it is possible to make a function that is faster, but which gives the same result (namely just take the first 10 chars).</p>
<p>Likewise it may be possible to make a faster function that gives the same output as a repeated hashing function. So your choice of hashing function is very important: as with the rot13 example it is not given that repeated hashing will improve security. If there is no research saying that the algorithm is designed for recursive use, then it is safer to assume that it will not give you added protection.</p>
<p>That said: For all but the simplest hashing functions it will most likely take cryptography experts to compute the faster functions, so if you are guarding against attackers that do not have access to cryptography experts it is probably safer in practice to use a repeated hashing function.</p>
</div></text><author><a href="/users/363028/ole-tange">Ole Tange</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>The concern about reducing the search space is mathematically correct, although the search space remains large enough that for all practical purposes (assuming you use salts), at 2^128.  However, since we are talking about passwords, the number of possible 16-character strings (alphanumeric, caps matter, a few symbols thrown in) is roughly 2^98, according to my back-of-the-envelope calculations.  So the perceived decrease in the search space is not really relevant.</p>
<p>Aside from that, there really is no difference, cryptographically speaking. </p>
<p>Although there is a crypto primitive called a "hash chain" -- a technique that allows you to do some cool tricks, like disclosing a signature key after it's been used, without sacrificing the integrity of the system -- given minimal time synchronization, this allows you to cleanly sidestep the problem of initial key distribution.  Basically, you precompute a large set of hashes of hashes - h(h(h(h....(h(k))...))) , use the nth value to sign, after a set interval, you send out the key, and sign it using key (n-1).  The recepients can now verify that you sent all the previous messages, and no one can fake your signature since the time period for which it is valid has passed.  </p>
<p>Re-hashing hundreds of thousands of times like Bill suggests is just a waste of your cpu.. use a longer key if you are concerned about people breaking 128 bits.</p>
</div></text><author><a href="/users/15962/squarecog">SquareCog</a></author><comments><comment><text><span class="comment-copy">Re-hashing is precisely about slowing down the hash. This is a key security feature in password-based cryptography. See the links for PCKS5 and PBKDF2.</span></text><author><a class="comment-user" href="/users/37020/orip" title="40,336 reputation">orip</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<h2>Yes.</h2>
<p>Absolutely <strong>do not</strong> use multiple iterations of a conventional hash function, like <code>md5(md5(md5(password)))</code>. At <em>best</em> you will be getting a marginal increase in security (a scheme like this offers hardly any protection against a GPU attack; just pipeline it.) At worst, you're reducing your hash space (and thus security) with every iteration you add. In security, it's wise to assume the worst.</p>
<p><strong>Do</strong> use a password has that's been <em>designed</em> by a competent cryptographer to be an effective password hash, and resistant to both brute-force and time-space attacks. These include bcrypt, scrypt, and in some situations PBKDF2. The glibc SHA-256-based hash is also acceptable.</p>
</div></text><author><a href="/users/152948/hobbs">hobbs</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>Double hashing makes sense to me only if I hash the password on the client, and then save the hash (with different salt) of that hash on the server.</p>
<p>That way even if someone hacked his way into the server (thereby ignoring the safety SSL provides), he still can't get to the clear passwords.</p>
<p>Yes he will have the data required to breach into the system, but he wouldn't be able to use that data to compromise outside accounts the user has. And people are known to use the same password for virtually anything.</p>
<p>The only way he could get to the clear passwords is installing a keygen on the client - and that's not your problem anymore.</p>
<p>So in short:</p>
<ol>
<li>The first hashing on the client protects your users in a 'server breach' scenario.</li>
<li>The second hashing on the server serves to protect your system if someone got a hold of your database backup, so he can't use those passwords to connect to your services.</li>
</ol>
</div></text><author><a href="/users/769137/vedran">Vedran</a></author><comments><comment><text><span class="comment-copy">+1 I was waiting to see an answer like this one, because I thought of the same scenario where you don't want to store plain-text password on the client, but also not send the final encrypted password over the wire to do a simple comparison to the DB.</span></text><author><a class="comment-user" href="/users/2454017/mark" title="113 reputation">Mark</a></author></comment></comments></answer><answer><text><div class="post-text" itemprop="text">
<p>I'm going to go out on a limb and say it's more secure in certain circumstances... don't downvote me yet though!</p>
<p>From a mathematical / cryptographical point of view, it's less secure, for reasons that I'm sure someone else will give you a clearer explanation of than I could.</p>
<p><strong>However</strong>, there exist large databases of MD5 hashes, which are more likely to contain the "password" text than the MD5 of it.  So by double-hashing you're reducing the effectiveness of those databases.</p>
<p>Of course, if you use a salt then this advantage (disadvantage?) goes away.</p>
</div></text><author><a href="/users/24181/greg">Greg</a></author><comments/></answer><answer><text><div class="post-text" itemprop="text">
<p>Double hashing is ugly because it's more than likely an attacker has built a table to come up with most hashes. Better is to salt your hashes, and mix hashes together. There are also new schemas to "sign" hashes (basically salting), but in a more secure manner. </p>
</div></text><author><a href="/users/10432/sargun-dhillon">Sargun Dhillon</a></author><comments/></answer></answers></post>